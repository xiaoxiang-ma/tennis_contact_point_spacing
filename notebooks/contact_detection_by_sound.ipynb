{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tennis Contact Detection by Sound\n\nDetect ball-racket contact frames using **pure audio analysis** — no visual ball tracking needed.\n\n**How it works:**\n1. Extract audio from your tennis video\n2. Apply a bandpass filter (1–4 kHz) to isolate the characteristic impact \"thump\"\n3. Compute the amplitude envelope and detect peaks above a noise-adaptive threshold\n4. **Shape analysis**: Measure each peak's duration (FWHM) and symmetry to distinguish short, symmetric ball impacts from long, asymmetric shoe screeches\n5. Map detected peaks back to video frames\n6. Display annotated debug frames for visual inspection\n\n**Why audio?** Tennis ball impacts produce a sharp, distinctive sound (5–20ms duration) in the 1–4 kHz frequency range. This is far more reliable than visual ball tracking which fails on blurry/occluded balls.\n\n**Peak shape filtering**: Ball contacts are short (~5–20ms) and symmetric. Shoe screeches are longer (~50–200ms) and asymmetric. The algorithm scores peaks using a weighted composite of amplitude (20%), narrowness (40%), and symmetry (40%), so a compact impact beats a loud screech.\n\n**Cells:**\n1. **Setup** — Install dependencies\n2. **Upload & Detect** — Upload video, run audio contact detection\n3. **Audio Debug** — Waveform, envelope, and peak shape metrics\n4. **Visual Inspection** — Debug frames showing each detected contact\n5. **Pose Analysis** — Analyze body positioning at selected contact (optional)\n6. **Download** — Download results\n7. **3D Contact Viewer** — Interactive 3D figure: skeleton, racket, ball contact point"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup - Install Dependencies\n",
    "import os, sys, shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/xiaoxiang-ma/tennis_contact_point_spacing.git\"\n",
    "REPO_DIR = \"/content/tennis_contact_point_spacing\"\n",
    "\n",
    "# Always re-clone to ensure latest code\n",
    "if os.path.exists(REPO_DIR):\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "!git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "!pip install -q -r {REPO_DIR}/requirements.txt\n",
    "\n",
    "# Clear any cached module imports from previous runs\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if mod_name.startswith((\"src.\", \"utils.\")):\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"\\nSetup complete! No GPU required — audio detection runs on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Upload Video & Detect Contacts by Sound\nimport numpy as np\nimport cv2\nfrom google.colab import files\nfrom IPython.display import display, Image as IPImage, HTML\nimport os\n\nfrom utils.video_io import load_video\nfrom src.contact_detection import detect_contacts, get_debug_audio_data, get_debug_candidates\n\n#@markdown ### Audio Detection Settings\n#@markdown **Bandpass Filter** — Tennis impacts are strongest in 1–4 kHz\nLOW_FREQ = 1000  #@param {type:\"integer\"}\nHIGH_FREQ = 4000  #@param {type:\"integer\"}\n\n#@markdown **Sensitivity** — Lower threshold factor = more sensitive (more detections, more false positives)\nPEAK_THRESHOLD_FACTOR = 3.0  #@param {type:\"slider\", min:1.5, max:6.0, step:0.5}\n\n#@markdown **Noise Floor Percentile** — Baseline noise level estimation\nNOISE_PERCENTILE = 75.0  #@param {type:\"slider\", min:50.0, max:95.0, step:5.0}\n\n#@markdown **Min Gap Between Contacts (ms)** — Suppress duplicate detections\nMIN_GAP_MS = 300  #@param {type:\"slider\", min:100, max:1000, step:50}\n\n#@markdown ### Peak Shape Filtering\n#@markdown **Max Impact FWHM (ms)** — Peaks wider than this are penalized (shoe screeches are typically 50–200ms)\nMAX_IMPACT_FWHM_MS = 40  #@param {type:\"slider\", min:15, max:80, step:5}\n\n#@markdown ### Other Settings\nSAMPLE_RATE = 22050  #@param {type:\"integer\"}\nDEBUG_MODE = True  #@param {type:\"boolean\"}\n\n# Create output directory\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Upload video ---\nprint(\"Upload your tennis video (MP4, MOV, etc.):\")\nuploaded = files.upload()\nvideo_filename = list(uploaded.keys())[0]\nvideo_path = os.path.join(\"/content\", video_filename)\nwith open(video_path, \"wb\") as f:\n    f.write(uploaded[video_filename])\n\n# --- Load video ---\nprint(f\"\\nLoading video: {video_filename}\")\nframes, metadata = load_video(video_path)\nfps = metadata[\"fps\"]\nprint(f\"  Resolution: {metadata['width']}x{metadata['height']}\")\nprint(f\"  Frame rate: {fps:.1f} fps\")\nprint(f\"  Duration: {metadata['duration_sec']:.2f}s ({len(frames)} frames)\")\n\n# --- Detect contacts via audio ---\nprint(f\"\\n{'='*60}\")\nprint(\"DETECTING CONTACTS BY SOUND\")\nprint(f\"{'='*60}\")\n\ncontacts = detect_contacts(\n    video_path=video_path,\n    fps=fps,\n    sample_rate=SAMPLE_RATE,\n    low_freq=LOW_FREQ,\n    high_freq=HIGH_FREQ,\n    min_gap_ms=MIN_GAP_MS,\n    noise_percentile=NOISE_PERCENTILE,\n    peak_threshold_factor=PEAK_THRESHOLD_FACTOR,\n    max_impact_fwhm_ms=MAX_IMPACT_FWHM_MS,\n    debug=DEBUG_MODE,\n)\n\n# --- Get audio debug data for plotting ---\naudio_data = get_debug_audio_data(\n    video_path, SAMPLE_RATE, LOW_FREQ, HIGH_FREQ,\n)\n\n# --- Get all candidates with shape metrics for debug ---\nall_candidates = get_debug_candidates(\n    video_path=video_path,\n    fps=fps,\n    sample_rate=SAMPLE_RATE,\n    low_freq=LOW_FREQ,\n    high_freq=HIGH_FREQ,\n    noise_percentile=NOISE_PERCENTILE,\n    peak_threshold_factor=PEAK_THRESHOLD_FACTOR,\n    max_impact_fwhm_ms=MAX_IMPACT_FWHM_MS,\n)\n\n# --- Display results ---\nprint(f\"\\n{'='*60}\")\nprint(f\"RESULTS: {len(contacts)} contact(s) detected\")\nprint(f\"{'='*60}\")\n\ncontact_info = []\nfor i, (frame_num, confidence, source) in enumerate(contacts):\n    time_sec = frame_num / fps\n    contact_info.append({\n        'index': i,\n        'frame': frame_num,\n        'time': time_sec,\n        'confidence': confidence,\n    })\n    print(f\"  Contact {i+1}: Frame {frame_num} ({time_sec:.2f}s) — confidence {confidence:.0%}\")\n\n# Show all candidates with shape analysis\nif all_candidates:\n    selected_frames = set(c['frame'] for c in contact_info)\n    print(f\"\\n--- All candidate peaks (shape analysis) ---\")\n    print(f\"{'Time':>7s} {'Frame':>6s} {'Amp':>8s} {'FWHM':>8s} {'Sym':>6s} {'Score':>7s}  Status\")\n    print(\"-\" * 65)\n    for cand in all_candidates:\n        is_selected = cand['frame'] in selected_frames\n        status = \"SELECTED\" if is_selected else \"rejected\"\n        marker = \">>\" if is_selected else \"  \"\n        print(f\"{marker}{cand['time_sec']:5.3f}s {cand['frame']:5d} \"\n              f\"{cand['amplitude']:8.4f} {cand['fwhm_ms']:6.1f}ms \"\n              f\"{cand['symmetry']:5.2f} {cand['score']:7.3f}  {status}\")\n\n# Store for subsequent cells\nANALYSIS_DATA = {\n    'frames': frames,\n    'fps': fps,\n    'metadata': metadata,\n    'contacts': contact_info,\n    'contacts_raw': contacts,\n    'audio_data': audio_data,\n    'all_candidates': all_candidates,\n    'video_path': video_path,\n}\n\nprint(f\"\\nNext: Run cell 3 to see the audio waveform, then cell 4 for debug frames.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Audio Debug — Waveform, Envelope & Peak Shape Analysis\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom IPython.display import display\n\nif 'ANALYSIS_DATA' not in dir():\n    raise ValueError(\"Please run cell 2 first!\")\n\naudio_data = ANALYSIS_DATA['audio_data']\ncontacts = ANALYSIS_DATA['contacts']\nall_candidates = ANALYSIS_DATA.get('all_candidates', [])\nfps = ANALYSIS_DATA['fps']\nsr = audio_data['sample_rate']\nraw_audio = audio_data['raw_audio']\nenvelope = audio_data['envelope']\nduration = audio_data['duration_sec']\n\n# Time axes\ntime_audio = np.arange(len(raw_audio)) / sr\ntime_env = np.arange(len(envelope)) / sr\n\n# Noise floor and threshold\nnoise_floor = np.percentile(envelope, NOISE_PERCENTILE)\nthreshold = noise_floor * PEAK_THRESHOLD_FACTOR\n\nselected_frames = set(c['frame'] for c in contacts)\n\nfig, axes = plt.subplots(4, 1, figsize=(16, 14), gridspec_kw={'height_ratios': [2, 3, 3, 2]})\n\n# --- Raw waveform ---\naxes[0].plot(time_audio, raw_audio, color='steelblue', linewidth=0.3, alpha=0.7)\naxes[0].set_ylabel('Amplitude')\naxes[0].set_title('Raw Audio Waveform')\nfor c in contacts:\n    axes[0].axvline(c['time'], color='red', linewidth=1.5, alpha=0.8, linestyle='--')\naxes[0].set_xlim(0, duration)\n\n# --- Filtered envelope with ALL candidates annotated ---\naxes[1].plot(time_env, envelope, color='darkorange', linewidth=0.5)\naxes[1].axhline(noise_floor, color='gray', linewidth=1, linestyle=':', label=f'Noise floor ({NOISE_PERCENTILE}th pctl)')\naxes[1].axhline(threshold, color='red', linewidth=1, linestyle='--', label=f'Threshold ({PEAK_THRESHOLD_FACTOR}x noise)')\naxes[1].set_ylabel('Envelope Amplitude')\naxes[1].set_title(f'Bandpass Filtered Envelope ({LOW_FREQ}–{HIGH_FREQ} Hz) — All Candidates')\naxes[1].legend(loc='upper right')\n\nfor cand in all_candidates:\n    is_sel = cand['frame'] in selected_frames\n    color = 'green' if is_sel else 'gray'\n    marker = 'v' if is_sel else 'x'\n    ms = 12 if is_sel else 8\n    axes[1].plot(cand['time_sec'], cand['amplitude'], marker=marker, color=color,\n                 markersize=ms, markeredgewidth=2, zorder=5)\n\n    # Show FWHM width as horizontal bar at half-max\n    half_amp = cand['amplitude'] / 2\n    fwhm_sec = cand['fwhm_ms'] / 1000\n    left_t = cand['time_sec'] - fwhm_sec / 2\n    right_t = cand['time_sec'] + fwhm_sec / 2\n    axes[1].plot([left_t, right_t], [half_amp, half_amp],\n                 color=color, linewidth=2, alpha=0.7)\n\n    label = f\"FWHM={cand['fwhm_ms']:.0f}ms\\nsym={cand['symmetry']:.2f}\\nscore={cand['score']:.2f}\"\n    va = 'bottom' if is_sel else 'top'\n    y_offset = cand['amplitude'] * 1.05 if is_sel else cand['amplitude'] * 0.7\n    axes[1].annotate(label, xy=(cand['time_sec'], y_offset),\n                     fontsize=7, color=color, fontweight='bold' if is_sel else 'normal',\n                     ha='center', va=va)\n\nsel_patch = mpatches.Patch(color='green', label='Selected (impact)')\nrej_patch = mpatches.Patch(color='gray', label='Rejected (screech/noise)')\naxes[1].legend(handles=[sel_patch, rej_patch], loc='upper left')\n\n# --- Selected contacts only ---\naxes[2].plot(time_env, envelope, color='darkorange', linewidth=0.5)\naxes[2].axhline(threshold, color='red', linewidth=1, linestyle='--', alpha=0.5)\naxes[2].set_ylabel('Envelope Amplitude')\naxes[2].set_title('Selected Contacts (highest composite score per window)')\n\nfor c in contacts:\n    axes[2].axvline(c['time'], color='red', linewidth=2, alpha=0.9)\n    axes[2].annotate(\n        f\"Contact {c['index']+1}\\nFrame {c['frame']}\\n{c['confidence']:.0%}\",\n        xy=(c['time'], threshold),\n        xytext=(c['time'] + duration * 0.01, threshold * 1.5),\n        fontsize=8, color='red', fontweight='bold',\n        arrowprops=dict(arrowstyle='->', color='red', lw=1),\n    )\n\n# --- Scoring breakdown bar chart ---\nif all_candidates:\n    x_labels = [f\"t={c['time_sec']:.2f}s\" for c in all_candidates]\n    x_pos = np.arange(len(all_candidates))\n    bar_colors = ['green' if c['frame'] in selected_frames else 'lightgray' for c in all_candidates]\n    edge_colors = ['darkgreen' if c['frame'] in selected_frames else 'gray' for c in all_candidates]\n\n    scores = [c['score'] for c in all_candidates]\n    axes[3].bar(x_pos, scores, color=bar_colors, edgecolor=edge_colors, linewidth=1.5)\n    axes[3].set_xticks(x_pos)\n    axes[3].set_xticklabels(x_labels, fontsize=8, rotation=30)\n    axes[3].set_ylabel('Composite Score')\n    axes[3].set_title('Impact Score: 20% amplitude + 40% narrowness + 40% symmetry')\n\n    # Add metric labels on bars\n    for i, cand in enumerate(all_candidates):\n        axes[3].text(i, cand['score'] + 0.01,\n                     f\"{cand['fwhm_ms']:.0f}ms\\nsym={cand['symmetry']:.2f}\",\n                     ha='center', va='bottom', fontsize=7, color='black')\n\naxes[0].set_xlim(0, duration)\naxes[1].set_xlim(0, duration)\naxes[2].set_xlim(0, duration)\naxes[2].set_xlabel('Time (seconds)')\n\nplt.tight_layout()\nplt.savefig(os.path.join(output_dir, 'audio_debug.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nAudio stats:\")\nprint(f\"  Duration: {duration:.2f}s\")\nprint(f\"  Sample rate: {sr} Hz\")\nprint(f\"  Noise floor: {noise_floor:.6f}\")\nprint(f\"  Detection threshold: {threshold:.6f}\")\nprint(f\"  Max impact FWHM: {MAX_IMPACT_FWHM_MS} ms\")\nprint(f\"  Candidates above threshold: {len(all_candidates)}\")\nprint(f\"  Selected contacts: {len(contacts)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Visual Inspection — Debug Frames at Each Contact\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display, Image as IPImage, HTML\n",
    "\n",
    "from src.visualization import annotate_contact_frame, save_annotated_frame\n",
    "\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "#@markdown ### Display Settings\n",
    "#@markdown **Frames before/after contact** to show for context\n",
    "CONTEXT_FRAMES = 2  #@param {type:\"slider\", min:0, max:5, step:1}\n",
    "\n",
    "contacts = ANALYSIS_DATA['contacts']\n",
    "frames = ANALYSIS_DATA['frames']\n",
    "fps = ANALYSIS_DATA['fps']\n",
    "num_frames = len(frames)\n",
    "\n",
    "if len(contacts) == 0:\n",
    "    print(\"No contacts detected. Try lowering PEAK_THRESHOLD_FACTOR in cell 2.\")\n",
    "else:\n",
    "    print(f\"Showing debug frames for {len(contacts)} detected contact(s)...\")\n",
    "    print(f\"Context: {CONTEXT_FRAMES} frame(s) before and after each contact\\n\")\n",
    "\n",
    "    for c in contacts:\n",
    "        idx = c['index']\n",
    "        frame_num = c['frame']\n",
    "        confidence = c['confidence']\n",
    "        time_sec = c['time']\n",
    "\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"CONTACT {idx+1}: Frame {frame_num} ({time_sec:.2f}s) | Confidence: {confidence:.0%}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Collect frames: context before, contact frame, context after\n",
    "        frame_range = range(\n",
    "            max(0, frame_num - CONTEXT_FRAMES),\n",
    "            min(num_frames, frame_num + CONTEXT_FRAMES + 1)\n",
    "        )\n",
    "\n",
    "        row_images = []\n",
    "        for f in frame_range:\n",
    "            img = frames[f].copy()\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            if f == frame_num:\n",
    "                # This is the contact frame — annotate it\n",
    "                img = annotate_contact_frame(img, f, fps, confidence)\n",
    "            else:\n",
    "                # Context frame — just add frame number\n",
    "                label = f\"Frame {f} ({f/fps:.2f}s)\"\n",
    "                cv2.putText(img, label, (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)\n",
    "\n",
    "            # Resize for display if too large\n",
    "            max_display_h = 300\n",
    "            if h > max_display_h:\n",
    "                scale = max_display_h / h\n",
    "                img = cv2.resize(img, (int(w * scale), max_display_h))\n",
    "\n",
    "            row_images.append(img)\n",
    "\n",
    "        # Concatenate horizontally\n",
    "        # Pad to same height if needed\n",
    "        max_h = max(im.shape[0] for im in row_images)\n",
    "        padded = []\n",
    "        for im in row_images:\n",
    "            if im.shape[0] < max_h:\n",
    "                pad = np.zeros((max_h - im.shape[0], im.shape[1], 3), dtype=np.uint8)\n",
    "                im = np.vstack([im, pad])\n",
    "            padded.append(im)\n",
    "\n",
    "        strip = np.hstack(padded)\n",
    "\n",
    "        # Save and display\n",
    "        strip_path = os.path.join(output_dir, f\"contact_{idx+1}_debug_strip.png\")\n",
    "        save_annotated_frame(strip, strip_path)\n",
    "\n",
    "        # Also save the contact frame alone at full resolution\n",
    "        contact_frame_annotated = annotate_contact_frame(\n",
    "            frames[frame_num].copy(), frame_num, fps, confidence\n",
    "        )\n",
    "        solo_path = os.path.join(output_dir, f\"contact_{idx+1}_frame_{frame_num}.png\")\n",
    "        save_annotated_frame(contact_frame_annotated, solo_path)\n",
    "\n",
    "        # Display strip\n",
    "        _, buf = cv2.imencode('.png', strip)\n",
    "        display(IPImage(data=buf.tobytes(), width=min(strip.shape[1], 1200)))\n",
    "        print()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Saved {len(contacts)} contact debug strips + full-res frames to {output_dir}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Pose Analysis at Selected Contact (Optional)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "\n",
    "from utils.coordinate_transforms import (\n",
    "    pelvis_origin_transform, estimate_ground_plane, apply_ground_plane\n",
    ")\n",
    "from src.pose_estimation import PoseEstimator\n",
    "from src.measurements import compute_measurements\n",
    "from src.visualization import (\n",
    "    draw_skeleton, draw_contact_point, draw_measurements, save_annotated_frame\n",
    ")\n",
    "\n",
    "#@markdown ### Select contact to analyze\n",
    "CONTACT_INDEX = 0  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Shot type (determines which wrist = contact point)\n",
    "SHOT_TYPE = \"right_forehand\"  #@param [\"right_forehand\", \"right_backhand\", \"left_forehand\", \"left_backhand\"]\n",
    "\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "contacts = ANALYSIS_DATA['contacts']\n",
    "if len(contacts) == 0:\n",
    "    raise ValueError(\"No contacts detected.\")\n",
    "if CONTACT_INDEX < 0 or CONTACT_INDEX >= len(contacts):\n",
    "    raise ValueError(f\"Invalid index. Valid range: 0-{len(contacts)-1}\")\n",
    "\n",
    "contact = contacts[CONTACT_INDEX]\n",
    "frame_num = contact['frame']\n",
    "frames = ANALYSIS_DATA['frames']\n",
    "fps = ANALYSIS_DATA['fps']\n",
    "\n",
    "contact_wrist_name = \"right_wrist\" if SHOT_TYPE.startswith(\"right\") else \"left_wrist\"\n",
    "\n",
    "print(f\"Analyzing Contact {CONTACT_INDEX + 1}\")\n",
    "print(f\"  Frame: {frame_num} ({contact['time']:.2f}s)\")\n",
    "print(f\"  Confidence: {contact['confidence']:.0%}\")\n",
    "print(f\"  Shot type: {SHOT_TYPE} -> using {contact_wrist_name}\")\n",
    "\n",
    "frame = frames[frame_num]\n",
    "\n",
    "# --- Pose estimation ---\n",
    "print(\"\\nEstimating pose...\")\n",
    "pose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n",
    "landmarks, raw_result = pose_estimator.process_frame(frame)\n",
    "\n",
    "if landmarks is None:\n",
    "    pose_estimator.close()\n",
    "    raise ValueError(\"No pose detected. Player may not be visible in this frame.\")\n",
    "\n",
    "pixel_lm = pose_estimator.get_pixel_landmarks(raw_result, frame.shape)\n",
    "pose_estimator.close()\n",
    "print(\"  Pose detected!\")\n",
    "\n",
    "# --- Contact point = wrist position ---\n",
    "if contact_wrist_name not in pixel_lm:\n",
    "    raise ValueError(f\"{contact_wrist_name} not detected in pose.\")\n",
    "\n",
    "contact_pixel = pixel_lm[contact_wrist_name]\n",
    "\n",
    "# --- Transform coordinates ---\n",
    "centered = pelvis_origin_transform(landmarks)\n",
    "ground_z = estimate_ground_plane(centered)\n",
    "adjusted = apply_ground_plane(centered, ground_z)\n",
    "\n",
    "wrist_3d = landmarks.get(contact_wrist_name, np.zeros(3))\n",
    "pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n",
    "contact_adjusted = wrist_3d - pelvis - np.array([0, 0, ground_z])\n",
    "\n",
    "# --- Measurements ---\n",
    "meas = compute_measurements(adjusted, contact_adjusted)\n",
    "meas[\"shot_type\"] = SHOT_TYPE\n",
    "meas[\"frame_num\"] = frame_num\n",
    "meas[\"contact_confidence\"] = contact['confidence']\n",
    "\n",
    "# --- Annotated frame ---\n",
    "annotated = frame.copy()\n",
    "annotated = draw_skeleton(annotated, pixel_lm, thickness=3)\n",
    "cx, cy = int(contact_pixel[0]), int(contact_pixel[1])\n",
    "draw_contact_point(annotated, cx, cy, radius=15)\n",
    "annotated = draw_measurements(annotated, meas, frame_num, fps)\n",
    "\n",
    "out_path = os.path.join(output_dir, f\"contact_{CONTACT_INDEX+1}_pose.png\")\n",
    "save_annotated_frame(annotated, out_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONTACT POINT MEASUREMENTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Lateral offset:      {meas.get('lateral_offset_cm', 0):>7.1f} cm\")\n",
    "print(f\"Forward/back:        {meas.get('forward_back_cm', 0):>7.1f} cm\")\n",
    "print(f\"Height above ground: {meas.get('height_above_ground_cm', 0):>7.1f} cm\")\n",
    "if 'shoulder_line_distance_cm' in meas:\n",
    "    print(f\"Shoulder distance:   {meas['shoulder_line_distance_cm']:>7.1f} cm\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "display(IPImage(filename=out_path, width=800))\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"measurements_contact_{CONTACT_INDEX+1}.csv\")\n",
    "pd.DataFrame([meas]).to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Download Results\n",
    "from google.colab import files as colab_files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/output\"\n",
    "all_files = glob.glob(os.path.join(output_dir, \"*\"))\n",
    "\n",
    "if not all_files:\n",
    "    print(\"No output files yet. Run the detection cells first.\")\n",
    "else:\n",
    "    print(\"Files available:\")\n",
    "    for f in sorted(all_files):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f\"  {os.path.basename(f)} ({size_kb:.1f} KB)\")\n",
    "\n",
    "    print(\"\\nDownloading...\")\n",
    "    for f in all_files:\n",
    "        colab_files.download(f)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 7. Interactive 3D Contact Point Viewer\n",
    "# Requires Cell 2 to have been run first (ANALYSIS_DATA must be in memory).\n",
    "#\n",
    "# For each detected contact:\n",
    "#   - Runs pose estimation on a ±window_frames frame window (smoothed)\n",
    "#   - Computes swing velocity from wrist trajectory\n",
    "#   - Tries to detect the ball via HSV color thresholding in that window\n",
    "#   - Builds a Plotly 3D figure with skeleton, racket, ball, ground plane\n",
    "#\n",
    "# Use the dropdowns to select a contact and shot type, then click \"Run →\".\n",
    "\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from src.pose_estimation import PoseEstimator\n",
    "from src.contact_window_analysis import analyze_contact_window\n",
    "from src.visualization_3d import create_contact_3d_figure\n",
    "\n",
    "if \"ANALYSIS_DATA\" not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "contacts = ANALYSIS_DATA[\"contacts\"]\n",
    "frames   = ANALYSIS_DATA[\"frames\"]\n",
    "fps      = ANALYSIS_DATA[\"fps\"]\n",
    "\n",
    "if len(contacts) == 0:\n",
    "    print(\"No contacts detected. Run cell 2 with your tennis video first.\")\n",
    "else:\n",
    "    # --- Widgets ---\n",
    "    contact_options = [\n",
    "        (f\"Contact {c['index']+1}  |  Frame {c['frame']}  ({c['time']:.2f}s)  conf {c['confidence']:.0%}\", i)\n",
    "        for i, c in enumerate(contacts)\n",
    "    ]\n",
    "    contact_dd = widgets.Dropdown(\n",
    "        options=contact_options,\n",
    "        description=\"Contact:\",\n",
    "        style={\"description_width\": \"80px\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    shot_dd = widgets.Dropdown(\n",
    "        options=[\"right_forehand\", \"right_backhand\", \"left_forehand\", \"left_backhand\"],\n",
    "        description=\"Shot type:\",\n",
    "        style={\"description_width\": \"80px\"},\n",
    "        layout=widgets.Layout(width=\"260px\"),\n",
    "    )\n",
    "\n",
    "    window_slider = widgets.IntSlider(\n",
    "        value=5, min=2, max=10, step=1,\n",
    "        description=\"Window ±N:\",\n",
    "        style={\"description_width\": \"80px\"},\n",
    "        layout=widgets.Layout(width=\"300px\"),\n",
    "    )\n",
    "\n",
    "    run_btn = widgets.Button(\n",
    "        description=\"Run →\",\n",
    "        button_style=\"primary\",\n",
    "        layout=widgets.Layout(width=\"100px\"),\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Cache: {(contact_index, shot_type, window_frames): analysis_dict}\n",
    "    _pose_cache = {}\n",
    "\n",
    "    def _run(_btn=None):\n",
    "        contact_idx  = contact_dd.value\n",
    "        shot_type    = shot_dd.value\n",
    "        window_n     = window_slider.value\n",
    "        contact_info = contacts[contact_idx]\n",
    "        frame_num    = contact_info[\"frame\"]\n",
    "        cache_key    = (contact_idx, shot_type, window_n)\n",
    "\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Analyzing Contact {contact_idx+1} — Frame {frame_num}  ({contact_info['time']:.2f}s)…\")\n",
    "\n",
    "            if cache_key in _pose_cache:\n",
    "                analysis = _pose_cache[cache_key]\n",
    "                print(\"  (using cached pose result)\")\n",
    "            else:\n",
    "                estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n",
    "                try:\n",
    "                    analysis = analyze_contact_window(\n",
    "                        frames=frames,\n",
    "                        contact_frame=frame_num,\n",
    "                        fps=fps,\n",
    "                        pose_estimator=estimator,\n",
    "                        shot_type=shot_type,\n",
    "                        window_frames=window_n,\n",
    "                    )\n",
    "                finally:\n",
    "                    estimator.close()\n",
    "                _pose_cache[cache_key] = analysis\n",
    "\n",
    "            if not analysis[\"landmarks_3d\"]:\n",
    "                print(\"  No pose detected in this window. Try a different contact or shot type.\")\n",
    "                return\n",
    "\n",
    "            # Quality summary\n",
    "            quality     = analysis[\"detection_quality\"]\n",
    "            n_dets      = len(analysis[\"ball_detections\"])\n",
    "            window_size = 2 * window_n\n",
    "            quality_msg = {\n",
    "                \"good\":     f\"Ball detection: good  ({n_dets} / {window_size} window frames)\",\n",
    "                \"partial\":  f\"Ball detection: partial  ({n_dets} / {window_size} window frames — one side only)\",\n",
    "                \"fallback\": f\"Ball detection: fallback to racket center  (0 / {window_size} usable detections)\",\n",
    "            }.get(quality, quality)\n",
    "\n",
    "            # Build and show figure\n",
    "            fig = create_contact_3d_figure(\n",
    "                analysis=analysis,\n",
    "                contact_info=contact_info,\n",
    "                title=f\"Contact Point — 3D View  ({shot_type})\",\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "            print()\n",
    "            print(quality_msg)\n",
    "\n",
    "    run_btn.on_click(_run)\n",
    "\n",
    "    controls = widgets.HBox([contact_dd, shot_dd, window_slider, run_btn])\n",
    "    display(controls, output_area)\n",
    "\n",
    "    # Auto-run on first display\n",
    "    _run()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}