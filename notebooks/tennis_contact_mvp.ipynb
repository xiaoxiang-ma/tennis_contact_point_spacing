{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis Contact Point Analysis - MVP\n",
    "\n",
    "Upload a tennis rally video to detect contact points, estimate 3D pose, and measure contact position relative to body landmarks.\n",
    "\n",
    "**Steps:**\n",
    "1. Run the **Setup** cell to install dependencies\n",
    "2. Run the **Upload & Process** cell to upload your video and run the pipeline\n",
    "3. View results and download annotated images + CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1. Setup - Install dependencies and clone repo\nimport os, sys, shutil\n\nREPO_URL = \"https://github.com/xiaoxiang-ma/tennis_contact_point_spacing.git\"\nREPO_DIR = \"/content/tennis_contact_point_spacing\"\n\n# Always re-clone to ensure latest code\nif os.path.exists(REPO_DIR):\n    shutil.rmtree(REPO_DIR)\n!git clone {REPO_URL} {REPO_DIR}\n\n!pip install -q -r {REPO_DIR}/requirements.txt\n\n# Clear any cached module imports from previous runs\nfor mod_name in list(sys.modules.keys()):\n    if mod_name.startswith((\"src.\", \"utils.\")):\n        del sys.modules[mod_name]\n\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Upload & Process Video\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom google.colab import files\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display, Image as IPImage\nimport os\n\nfrom utils.video_io import load_video\nfrom utils.coordinate_transforms import (\n    pelvis_origin_transform, estimate_ground_plane,\n    apply_ground_plane, estimate_player_height_scale\n)\nfrom src.ball_detection import BallTracker\nfrom src.pose_estimation import PoseEstimator, LANDMARK_MAP\nfrom src.contact_detection import detect_contacts\nfrom src.measurements import compute_measurements\nfrom src.visualization import (\n    annotate_contact_frame, save_annotated_frame\n)\n\n# --- Upload video ---\nprint(\"Upload your tennis video (MP4, MOV, AVI):\")\nuploaded = files.upload()\nvideo_filename = list(uploaded.keys())[0]\nvideo_path = os.path.join(\"/content\", video_filename)\nwith open(video_path, \"wb\") as f:\n    f.write(uploaded[video_filename])\n\n# --- Load video ---\nprint(f\"\\nLoading video: {video_filename}\")\nframes, meta = load_video(video_path)\nfps = meta[\"fps\"]\nprint(f\"  {len(frames)} frames, {meta['width']}x{meta['height']}, {fps:.1f} fps, {meta['duration_sec']:.1f}s\")\n\n# --- Ball detection ---\nprint(\"\\nDetecting ball...\")\ntracker = BallTracker()  # HSV fallback (no TrackNet weights)\nball_detections = tracker.detect_all(frames, progress=True)\nprint(f\"  Ball detected in {len(ball_detections)}/{len(frames)} frames\")\n\n# Quick diagnostic: show ball position range\nif ball_detections:\n    xs = [d[1] for d in ball_detections]\n    ys = [d[2] for d in ball_detections]\n    print(f\"  Ball x range: {min(xs):.0f}-{max(xs):.0f}, y range: {min(ys):.0f}-{max(ys):.0f}\")\n\n# --- Pose estimation ---\nprint(\"\\nEstimating pose...\")\npose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n\nwrist_positions = {}  # frame_num -> (x, y) in pixels\npose_cache = {}  # frame_num -> (world_landmarks, raw_result)\n\n# Process ALL frames (short video, and we need full wrist trajectory)\nfor i in tqdm(range(len(frames)), desc=\"Pose estimation\"):\n    landmarks, result = pose_estimator.process_frame(frames[i])\n    if landmarks is not None:\n        pose_cache[i] = (landmarks, result)\n\n        # Get pixel-space wrist positions for contact detection\n        pixel_lm = pose_estimator.get_pixel_landmarks(result, frames[i].shape)\n        if pixel_lm is not None:\n            h, w = frames[i].shape[:2]\n            rw = pixel_lm.get(\"right_wrist\")\n            lw = pixel_lm.get(\"left_wrist\")\n            if rw and lw:\n                mid_x = w / 2\n                if abs(rw[0] - mid_x) > abs(lw[0] - mid_x):\n                    wrist_positions[i] = (float(rw[0]), float(rw[1]))\n                else:\n                    wrist_positions[i] = (float(lw[0]), float(lw[1]))\n\npose_estimator.close()\nprint(f\"  Pose estimated for {len(pose_cache)} frames\")\nprint(f\"  Wrist positions for {len(wrist_positions)} frames\")\n\n# --- Contact detection ---\nprint(\"\\nDetecting contacts...\")\ncontacts = detect_contacts(\n    ball_detections, fps,\n    wrist_positions=wrist_positions,\n    velocity_spike_threshold=1.5,\n    wrist_proximity_px=300,\n    min_frame_gap=int(fps * 0.3),  # at least 0.3s apart\n    debug=True,\n)\nprint(f\"  Found {len(contacts)} contact(s)\")\n\nif not contacts:\n    print(\"\\nNo contacts detected. Try adjusting camera angle or lighting.\")\n\n# --- Process each contact ---\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\n\nall_measurements = []\n\n# Create a fresh pose estimator for any missing contact frames\npe2 = PoseEstimator(static_image_mode=True, model_complexity=2)\n\nfor contact_frame, confidence in contacts:\n    if contact_frame not in pose_cache:\n        landmarks, result = pe2.process_frame(frames[contact_frame])\n        if landmarks is None:\n            print(f\"  Frame {contact_frame}: no pose detected, skipping\")\n            continue\n        pose_cache[contact_frame] = (landmarks, result)\n\n    landmarks, raw_result = pose_cache[contact_frame]\n\n    # Determine dominant wrist (the one further extended from pelvis)\n    lw = landmarks.get(\"left_wrist\", np.zeros(3))\n    rw = landmarks.get(\"right_wrist\", np.zeros(3))\n    pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n    if np.linalg.norm(rw - pelvis) > np.linalg.norm(lw - pelvis):\n        contact_wrist_name = \"right_wrist\"\n        contact_point = rw\n    else:\n        contact_wrist_name = \"left_wrist\"\n        contact_point = lw\n\n    # Transform to pelvis-centered, ground-adjusted coordinates\n    centered = pelvis_origin_transform(landmarks)\n    ground_z = estimate_ground_plane(centered)\n    adjusted = apply_ground_plane(centered, ground_z)\n\n    # Contact point in same coordinate system\n    contact_adjusted = contact_point - pelvis - np.array([0, 0, ground_z])\n\n    # Compute measurements\n    meas = compute_measurements(adjusted, contact_adjusted)\n    meas[\"frame_number\"] = contact_frame\n    meas[\"timestamp\"] = contact_frame / fps\n    meas[\"confidence\"] = confidence\n    meas[\"contact_x\"] = contact_point[0]\n    meas[\"contact_y\"] = contact_point[1]\n    meas[\"contact_z\"] = contact_point[2]\n    all_measurements.append(meas)\n\n    # Visualize\n    pixel_lm = pe2.get_pixel_landmarks(raw_result, frames[contact_frame].shape)\n    if pixel_lm is None:\n        # Fallback: re-run pose just for pixel landmarks\n        _, r2 = pe2.process_frame(frames[contact_frame])\n        pixel_lm = pe2.get_pixel_landmarks(r2, frames[contact_frame].shape)\n\n    if pixel_lm:\n        annotated = annotate_contact_frame(\n            frames[contact_frame], pixel_lm, contact_wrist_name,\n            meas, contact_frame, fps\n        )\n        out_path = os.path.join(output_dir, f\"contact_frame_{contact_frame}.png\")\n        save_annotated_frame(annotated, out_path)\n    print(f\"  Contact at frame {contact_frame} (t={contact_frame/fps:.2f}s, conf={confidence:.2f})\")\n\npe2.close()\n\n# Save CSV\nif all_measurements:\n    video_stem = os.path.splitext(video_filename)[0]\n    csv_path = os.path.join(output_dir, f\"measurements_{video_stem}.csv\")\n    df = pd.DataFrame(all_measurements)\n    first_cols = [\"frame_number\", \"timestamp\", \"confidence\",\n                  \"contact_x\", \"contact_y\", \"contact_z\"]\n    other_cols = [c for c in df.columns if c not in first_cols]\n    df = df[first_cols + sorted(other_cols)]\n    df.to_csv(csv_path, index=False)\n    print(f\"\\nMeasurements saved to {csv_path}\")\n\nprint(\"\\nProcessing complete!\")"
  },
  {
   "cell_type": "code",
   "source": "#@title 2b. Visual Debug â€” Ball detections, wrist positions, velocity plot\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom IPython.display import display, Image as IPImage\nfrom src.contact_detection import compute_ball_velocity\n\n# --- 1. Ball trajectory plot ---\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nball_frames_list = [d[0] for d in ball_detections]\nball_xs = [d[1] for d in ball_detections]\nball_ys = [d[2] for d in ball_detections]\n\naxes[0, 0].plot(ball_frames_list, ball_xs, 'g.-', markersize=2, label='ball x')\nif wrist_positions:\n    wf = sorted(wrist_positions.keys())\n    axes[0, 0].plot(wf, [wrist_positions[f][0] for f in wf], 'b.-', markersize=2, alpha=0.5, label='wrist x')\naxes[0, 0].set_title(\"X position over time\")\naxes[0, 0].set_xlabel(\"Frame\")\naxes[0, 0].set_ylabel(\"Pixels\")\naxes[0, 0].legend()\n\naxes[0, 1].plot(ball_frames_list, ball_ys, 'g.-', markersize=2, label='ball y')\nif wrist_positions:\n    axes[0, 1].plot(wf, [wrist_positions[f][1] for f in wf], 'b.-', markersize=2, alpha=0.5, label='wrist y')\naxes[0, 1].set_title(\"Y position over time\")\naxes[0, 1].set_xlabel(\"Frame\")\naxes[0, 1].set_ylabel(\"Pixels\")\naxes[0, 1].legend()\n\n# Velocity plot\nvels = compute_ball_velocity(ball_detections, fps)\nif vels:\n    vel_frames = [v[0] for v in vels]\n    vel_speeds = [v[3] for v in vels]\n    axes[1, 0].plot(vel_frames, vel_speeds, 'r.-', markersize=2)\n    axes[1, 0].set_title(\"Ball speed (px/s)\")\n    axes[1, 0].set_xlabel(\"Frame\")\n    axes[1, 0].set_ylabel(\"Speed\")\n    # Mark contacts if any\n    for cf, cc in contacts:\n        axes[1, 0].axvline(x=cf, color='blue', alpha=0.7, linestyle='--', label=f'contact {cf}')\n    if contacts:\n        axes[1, 0].legend()\n\n# Ball-wrist distance plot\nif wrist_positions:\n    ball_pos_dict = {d[0]: (d[1], d[2]) for d in ball_detections}\n    common_frames = sorted(set(ball_pos_dict.keys()) & set(wrist_positions.keys()))\n    if common_frames:\n        dists = []\n        for f in common_frames:\n            bx, by = ball_pos_dict[f]\n            wx, wy = wrist_positions[f]\n            dists.append(np.sqrt((bx - wx)**2 + (by - wy)**2))\n        axes[1, 1].plot(common_frames, dists, 'm.-', markersize=2)\n        axes[1, 1].set_title(\"Ball-wrist distance (px)\")\n        axes[1, 1].set_xlabel(\"Frame\")\n        axes[1, 1].set_ylabel(\"Distance\")\n        axes[1, 1].axhline(y=300, color='gray', linestyle=':', label='proximity threshold')\n        for cf, cc in contacts:\n            axes[1, 1].axvline(x=cf, color='blue', alpha=0.7, linestyle='--')\n        axes[1, 1].legend()\n\nplt.tight_layout()\nplt.savefig(\"/content/output/debug_plots.png\", dpi=120)\nplt.close()\ndisplay(IPImage(filename=\"/content/output/debug_plots.png\", width=900))\n\n# --- 2. Annotated sample frames showing ball (green) and wrist (blue) ---\nprint(\"\\nSample frames with ball (green circle) and wrist (blue circle):\")\nsample_indices = np.linspace(0, len(frames) - 1, min(8, len(frames)), dtype=int)\n\nfig2, axes2 = plt.subplots(2, 4, figsize=(20, 10))\nball_pos_dict = {d[0]: (d[1], d[2]) for d in ball_detections}\n\nfor idx, ax in zip(sample_indices, axes2.flat):\n    frame_rgb = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB).copy()\n\n    # Draw ball detection\n    if idx in ball_pos_dict:\n        bx, by = int(ball_pos_dict[idx][0]), int(ball_pos_dict[idx][1])\n        cv2.circle(frame_rgb, (bx, by), 12, (0, 255, 0), 3)\n        cv2.putText(frame_rgb, \"ball\", (bx + 15, by - 5),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Draw wrist position\n    if idx in wrist_positions:\n        wx, wy = int(wrist_positions[idx][0]), int(wrist_positions[idx][1])\n        cv2.circle(frame_rgb, (wx, wy), 12, (50, 50, 255), 3)\n        cv2.putText(frame_rgb, \"wrist\", (wx + 15, wy - 5),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 50, 255), 2)\n\n    ax.imshow(frame_rgb)\n    ax.set_title(f\"Frame {idx} (t={idx/fps:.2f}s)\")\n    ax.axis(\"off\")\n\n# Hide unused subplots\nfor i in range(len(sample_indices), len(axes2.flat)):\n    axes2.flat[i].axis(\"off\")\n\nplt.tight_layout()\nplt.savefig(\"/content/output/debug_frames.png\", dpi=120)\nplt.close()\ndisplay(IPImage(filename=\"/content/output/debug_frames.png\", width=900))\n\nprint(\"\\nCheck: Is the green circle on the actual tennis ball?\")\nprint(\"If not, the HSV color detection is picking up the wrong object.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Results - View annotated frames & download\n",
    "import glob\n",
    "from IPython.display import display, Image as IPImage, HTML\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = \"/content/output\"\n",
    "\n",
    "# Display annotated images\n",
    "png_files = sorted(glob.glob(os.path.join(output_dir, \"contact_frame_*.png\")))\n",
    "if png_files:\n",
    "    print(f\"Found {len(png_files)} contact frame(s):\\n\")\n",
    "    for png_path in png_files:\n",
    "        print(os.path.basename(png_path))\n",
    "        display(IPImage(filename=png_path, width=800))\n",
    "        print()\n",
    "else:\n",
    "    print(\"No annotated frames found.\")\n",
    "\n",
    "# Display measurements table\n",
    "csv_files = glob.glob(os.path.join(output_dir, \"measurements_*.csv\"))\n",
    "if csv_files:\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    print(\"\\nMeasurements:\")\n",
    "    display(df)\n",
    "\n",
    "# Download links\n",
    "print(\"\\n--- Download Files ---\")\n",
    "from google.colab import files as colab_files\n",
    "for f in png_files + csv_files:\n",
    "    colab_files.download(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}