{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tennis Contact Point Analysis\n\nUpload a **video** of your tennis rally to automatically detect contact frames and analyze body positioning.\n\n**How it works:**\n1. **TrackNetV4** (deep learning) tracks the ball with motion attention for improved detection near players\n2. **Audio analysis** detects impact sounds (ball hitting racket)\n3. Both signals are **fused** for robust contact detection\n4. For each contact, we analyze pose and measure contact point spacing\n\n**TrackNetV4 Enhancement:**\nThe motion attention module (based on [ICASSP 2025 paper](https://arxiv.org/abs/2409.14543)) improves ball tracking when:\n- Ball is near the player/racket (occlusion scenarios)\n- Using custom camera angles (different from broadcast training data)\n- Low contrast or challenging lighting conditions\n\n**Cells:**\n1. **Setup** - Install dependencies\n2. **Upload & Detect** - Upload video and run contact detection (with motion attention options)\n3. **Generate Diagnostic Video** - Creates video with overlays showing detection signals\n4. **Text Diagnostics** - Frame-by-frame text output\n5. **Analyze Contact** - Detailed analysis of a selected contact\n6. **Batch Analysis** - Analyze all contacts at once\n7. **Download** - Download results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup - Install Dependencies\n",
    "import os, sys, shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/xiaoxiang-ma/tennis_contact_point_spacing.git\"\n",
    "REPO_DIR = \"/content/tennis_contact_point_spacing\"\n",
    "\n",
    "# Always re-clone to ensure latest code\n",
    "if os.path.exists(REPO_DIR):\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "!git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "!pip install -q -r {REPO_DIR}/requirements.txt\n",
    "\n",
    "# Clear any cached module imports from previous runs\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if mod_name.startswith((\"src.\", \"utils.\")):\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Create weights directory\n",
    "os.makedirs(os.path.join(REPO_DIR, \"weights\"), exist_ok=True)\n",
    "\n",
    "print(\"\\nSetup complete!\")\n",
    "print(\"Note: TrackNet weights will be downloaded automatically on first use (~50MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Upload Video & Detect Contacts\nimport numpy as np\nimport cv2\nfrom google.colab import files\nfrom IPython.display import display, Image as IPImage, HTML\nimport os\n\nfrom utils.video_io import load_video\nfrom src.tracknet import TrackNetDetector, TrackNetV4Detector\nfrom src.contact_detection import detect_contacts, get_contact_ball_position\n\n#@markdown ### Shot Settings\nSHOT_TYPE = \"right_forehand\"  #@param [\"right_forehand\", \"right_backhand\", \"left_forehand\", \"left_backhand\"]\n\n#@markdown ### Detection Settings\n#@markdown **Ball Detection Confidence** (lower = more detections but more noise)\nCONFIDENCE_THRESHOLD = 0.3  #@param {type:\"slider\", min:0.1, max:0.9, step:0.1}\n\n#@markdown **Fill Gaps** - Interpolate ball position through occlusion (highly recommended)\nFILL_GAPS = True  #@param {type:\"boolean\"}\n\n#@markdown **Max Gap to Fill** - Maximum consecutive missing frames to interpolate\nMAX_GAP_FRAMES = 10  #@param {type:\"slider\", min:3, max:20, step:1}\n\n#@markdown ### Motion Attention (TrackNetV4)\n#@markdown Enable motion attention to improve detection near players/rackets\nUSE_MOTION_ATTENTION = True  #@param {type:\"boolean\"}\n\n#@markdown **Motion Boost** - How much to boost detections in moving areas (higher = stronger)\nMOTION_BOOST = 2.0  #@param {type:\"slider\", min:1.0, max:4.0, step:0.5}\n\n#@markdown **Motion Min Attention** - Minimum weight for static areas (lower = more suppression)\nMOTION_MIN_ATTENTION = 0.3  #@param {type:\"slider\", min:0.1, max:0.5, step:0.1}\n\n#@markdown ### Other Settings\nUSE_AUDIO = True  #@param {type:\"boolean\"}\nDEBUG_MODE = True  #@param {type:\"boolean\"}\nSAVE_DEBUG_FRAMES = False  #@param {type:\"boolean\"}\n\n# Create output directory\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Upload video ---\nprint(\"Upload your tennis video (MP4, MOV, etc.):\")\nuploaded = files.upload()\nvideo_filename = list(uploaded.keys())[0]\nvideo_path = os.path.join(\"/content\", video_filename)\nwith open(video_path, \"wb\") as f:\n    f.write(uploaded[video_filename])\n\n# --- Load video ---\nprint(f\"\\nLoading video: {video_filename}\")\nframes, metadata = load_video(video_path)\nfps = metadata[\"fps\"]\nprint(f\"  Resolution: {metadata['width']}x{metadata['height']}\")\nprint(f\"  Frame rate: {fps:.1f} fps\")\nprint(f\"  Duration: {metadata['duration_sec']:.2f}s ({len(frames)} frames)\")\n\n# --- Initialize TrackNet ---\nif USE_MOTION_ATTENTION:\n    print(f\"\\nInitializing TrackNetV4 with motion attention...\")\n    print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n    print(f\"  Motion boost: {MOTION_BOOST}x, Min attention: {MOTION_MIN_ATTENTION}\")\n    tracknet = TrackNetV4Detector(\n        weights_path=None,  # Auto-download\n        device=None,  # Auto-detect GPU/CPU\n        confidence_threshold=CONFIDENCE_THRESHOLD,\n        save_debug_frames=SAVE_DEBUG_FRAMES,\n        debug_output_dir=os.path.join(output_dir, \"tracknet_debug\"),\n        motion_boost=MOTION_BOOST,\n        motion_min_attention=MOTION_MIN_ATTENTION,\n        use_motion_attention=True,\n    )\nelse:\n    print(f\"\\nInitializing TrackNet (confidence threshold: {CONFIDENCE_THRESHOLD})...\")\n    tracknet = TrackNetDetector(\n        weights_path=None,  # Auto-download\n        device=None,  # Auto-detect GPU/CPU\n        confidence_threshold=CONFIDENCE_THRESHOLD,\n        save_debug_frames=SAVE_DEBUG_FRAMES,\n        debug_output_dir=os.path.join(output_dir, \"tracknet_debug\"),\n    )\n\n# Check if weights loaded successfully\nif not tracknet.weights_loaded:\n    print(\"\\n\" + \"!\"*60)\n    print(\"WARNING: TrackNet weights failed to load!\")\n    print(\"Ball detection will NOT work properly.\")\n    print(\"Check the error message above for details.\")\n    print(\"!\"*60 + \"\\n\")\n\n# --- Detect contacts ---\nprint(\"\\nDetecting contacts (this may take a few minutes)...\")\nif FILL_GAPS:\n    print(f\"  Gap filling enabled (max {MAX_GAP_FRAMES} frames)\")\n\ncontacts, ball_detections = detect_contacts(\n    video_path=video_path,\n    frames=frames,\n    fps=fps,\n    tracknet_detector=tracknet,\n    use_audio=USE_AUDIO,\n    fill_gaps=FILL_GAPS,\n    max_gap_frames=MAX_GAP_FRAMES,\n    debug=DEBUG_MODE,\n    save_debug_frames=SAVE_DEBUG_FRAMES,\n)\n\n# --- Build trajectory for diagnostics ---\ntrajectory = tracknet.get_ball_trajectory(ball_detections)\n\n# --- Display results ---\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"CONTACT DETECTION RESULTS\")\nprint(f\"=\"*60)\nprint(f\"Ball positions available: {len(ball_detections)}/{len(frames)} frames ({100*len(ball_detections)/len(frames):.1f}%)\")\nif FILL_GAPS:\n    print(f\"  (includes interpolated positions through gaps)\")\nif USE_MOTION_ATTENTION:\n    print(f\"Motion attention: ENABLED (boost={MOTION_BOOST}x)\")\n\nprint(f\"\\nDetected {len(contacts)} contact(s):\")\nprint()\n\n# Store contact info for later use\ncontact_info = []\nfor i, (frame_num, confidence, source) in enumerate(contacts):\n    time_sec = frame_num / fps\n    ball_pos, ball_method = get_contact_ball_position(frame_num, ball_detections)\n    \n    contact_info.append({\n        'index': i,\n        'frame': frame_num,\n        'time': time_sec,\n        'confidence': confidence,\n        'source': source,\n        'ball_pos': ball_pos,\n        'ball_method': ball_method,\n    })\n    \n    print(f\"  Contact {i+1}: Frame {frame_num} ({time_sec:.2f}s)\")\n    print(f\"    Confidence: {confidence:.0%} (source: {source})\")\n    if ball_pos:\n        print(f\"    Ball position: ({ball_pos[0]:.0f}, {ball_pos[1]:.0f}) [{ball_method}]\")\n    else:\n        print(f\"    Ball position: Not available\")\n    print()\n\n# Store for next cells\nANALYSIS_DATA = {\n    'frames': frames,\n    'fps': fps,\n    'metadata': metadata,\n    'contacts': contact_info,\n    'contacts_raw': contacts,  # Keep raw format for video generation\n    'ball_detections': ball_detections,\n    'trajectory': trajectory,\n    'shot_type': SHOT_TYPE,\n    'video_path': video_path,\n    'motion_attention_enabled': USE_MOTION_ATTENTION,\n}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Next: Run the DIAGNOSTIC VIDEO cell to visualize detection\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Generate Diagnostic Video (Recommended)\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from google.colab import files as colab_files\n",
    "\n",
    "from src.visualization import create_diagnostic_video\n",
    "\n",
    "#@markdown ### Settings\n",
    "#@markdown Set the known real contact frame (if you know it) to highlight in yellow:\n",
    "KNOWN_CONTACT_FRAME = 81  #@param {type:\"integer\"}\n",
    "SHOW_TRAJECTORY_TRAIL = True  #@param {type:\"boolean\"}\n",
    "TRAJECTORY_TAIL_FRAMES = 30  #@param {type:\"integer\"}\n",
    "\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "output_dir = \"/content/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"diagnostic_video.mp4\")\n",
    "\n",
    "print(\"Generating diagnostic video with overlays...\")\n",
    "print(\"  - Ball position (green circle)\")\n",
    "print(\"  - Velocity vector (yellow arrow)\")\n",
    "print(\"  - Detection signals: SPIKE (cyan), REVERSAL (magenta), DECEL (orange)\")\n",
    "print(\"  - Confidence meter\")\n",
    "print(\"  - Detected contacts (red border)\")\n",
    "if KNOWN_CONTACT_FRAME:\n",
    "    print(f\"  - Known contact frame {KNOWN_CONTACT_FRAME} (yellow border)\")\n",
    "print()\n",
    "\n",
    "create_diagnostic_video(\n",
    "    frames=ANALYSIS_DATA['frames'],\n",
    "    ball_detections=ANALYSIS_DATA['ball_detections'],\n",
    "    contacts=ANALYSIS_DATA['contacts_raw'],\n",
    "    fps=ANALYSIS_DATA['fps'],\n",
    "    output_path=output_path,\n",
    "    known_contact_frame=KNOWN_CONTACT_FRAME if KNOWN_CONTACT_FRAME > 0 else None,\n",
    "    show_trajectory=SHOW_TRAJECTORY_TRAIL,\n",
    "    trajectory_tail=TRAJECTORY_TAIL_FRAMES,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VIDEO OVERLAY LEGEND\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "TOP-LEFT PANEL:\n",
    "  Frame/Time     - Current frame number and timestamp\n",
    "  Ball           - DETECTED (green) or MISSING (red)\n",
    "  Speed          - Current ball speed in px/s\n",
    "  Signal boxes   - SPIKE (cyan), REVERSAL (magenta), DECEL (orange)\n",
    "  Conf bar       - Detection confidence (green=high, yellow=medium, orange=low)\n",
    "\n",
    "ON FRAME:\n",
    "  Green circle   - Ball position (size = confidence)\n",
    "  Yellow arrow   - Velocity direction\n",
    "  Green trail    - Recent ball trajectory\n",
    "  RED border     - Frame detected as contact\n",
    "  YELLOW border  - Known real contact frame (for comparison)\n",
    "\"\"\")\n",
    "\n",
    "# Display video in notebook\n",
    "from base64 import b64encode\n",
    "video_data = open(output_path, 'rb').read()\n",
    "video_b64 = b64encode(video_data).decode()\n",
    "display(HTML(f'''\n",
    "<video width=\"800\" controls>\n",
    "  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\">\n",
    "</video>\n",
    "'''))\n",
    "\n",
    "print(f\"\\nVideo saved to: {output_path}\")\n",
    "print(\"Downloading...\")\n",
    "colab_files.download(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Text Diagnostics - Frame-by-Frame Analysis (Optional)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from src.contact_detection import compute_ball_velocity, debug_frame_region\n",
    "\n",
    "#@markdown ### Diagnostic Settings\n",
    "KNOWN_CONTACT_FRAME = 81  #@param {type:\"integer\"}\n",
    "SHOW_ALL_FRAMES = False  #@param {type:\"boolean\"}\n",
    "FOCUS_WINDOW = 15  #@param {type:\"integer\"}\n",
    "\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "ball_detections = ANALYSIS_DATA['ball_detections']\n",
    "trajectory = ANALYSIS_DATA['trajectory']\n",
    "fps = ANALYSIS_DATA['fps']\n",
    "contacts = ANALYSIS_DATA['contacts']\n",
    "num_frames = len(ANALYSIS_DATA['frames'])\n",
    "\n",
    "# Compute velocities\n",
    "velocities = compute_ball_velocity(trajectory, fps)\n",
    "vel_dict = {v[0]: {'vx': v[1], 'vy': v[2], 'speed': v[3]} for v in velocities}\n",
    "\n",
    "# Reference speed for spike detection\n",
    "speeds = np.array([v[3] for v in velocities])\n",
    "nonzero_speeds = speeds[speeds > 1e-3]\n",
    "ref_speed = np.median(nonzero_speeds) if len(nonzero_speeds) > 0 else 100\n",
    "spike_threshold = ref_speed * 2.0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC: FRAME-BY-FRAME CONTACT DETECTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nVideo: {num_frames} frames at {fps:.1f} fps\")\n",
    "print(f\"Ball positions: {len(ball_detections)} frames ({100*len(ball_detections)/num_frames:.1f}%)\")\n",
    "print(f\"Velocity samples: {len(velocities)}\")\n",
    "print(f\"\\nReference speed (median): {ref_speed:.1f} px/s\")\n",
    "print(f\"Spike threshold (2x median): {spike_threshold:.1f} px/s\")\n",
    "\n",
    "# Build frame-by-frame data\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"FRAME-BY-FRAME VELOCITY ANALYSIS\")\n",
    "if not SHOW_ALL_FRAMES:\n",
    "    print(f\"(Showing frames {KNOWN_CONTACT_FRAME - FOCUS_WINDOW} to {KNOWN_CONTACT_FRAME + FOCUS_WINDOW})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Frame':>6} {'Time':>7} {'Ball?':>6} {'Speed':>10} {'Vx':>8} {'Vy':>8} {'Spike':>7} {'Reversal':>9} {'Decel':>7} {'Conf':>6} {'Notes'}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Determine frame range\n",
    "if SHOW_ALL_FRAMES:\n",
    "    frame_range = range(num_frames)\n",
    "else:\n",
    "    frame_range = range(\n",
    "        max(0, KNOWN_CONTACT_FRAME - FOCUS_WINDOW),\n",
    "        min(num_frames, KNOWN_CONTACT_FRAME + FOCUS_WINDOW + 1)\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "prev_vel = None\n",
    "\n",
    "for f in frame_range:\n",
    "    time_sec = f / fps\n",
    "    \n",
    "    # Ball detection status\n",
    "    ball_detected = f in ball_detections\n",
    "    ball_str = \"YES\" if ball_detected else \"---\"\n",
    "    \n",
    "    # Velocity data\n",
    "    if f in vel_dict:\n",
    "        v = vel_dict[f]\n",
    "        speed = v['speed']\n",
    "        vx, vy = v['vx'], v['vy']\n",
    "        speed_str = f\"{speed:8.1f}\"\n",
    "        vx_str = f\"{vx:+7.1f}\"\n",
    "        vy_str = f\"{vy:+7.1f}\"\n",
    "        \n",
    "        # Spike detection\n",
    "        is_spike = speed > spike_threshold\n",
    "        spike_str = f\"{speed/ref_speed:.1f}x\" if is_spike else \"\"\n",
    "        \n",
    "        # Reversal detection (need previous velocity)\n",
    "        reversal_str = \"\"\n",
    "        reversal_val = 0\n",
    "        if prev_vel is not None:\n",
    "            dot = prev_vel['vx'] * vx + prev_vel['vy'] * vy\n",
    "            if dot < 0:\n",
    "                mag0 = np.sqrt(prev_vel['vx']**2 + prev_vel['vy']**2)\n",
    "                mag1 = np.sqrt(vx**2 + vy**2)\n",
    "                cos_angle = dot / (mag0 * mag1 + 1e-6)\n",
    "                reversal_val = max(0, -cos_angle)\n",
    "                reversal_str = f\"{reversal_val:.2f}\"\n",
    "        \n",
    "        # Deceleration detection\n",
    "        decel_str = \"\"\n",
    "        decel_val = 0\n",
    "        if prev_vel is not None:\n",
    "            prev_speed = prev_vel['speed']\n",
    "            if prev_speed > ref_speed and speed < prev_speed * 0.5:\n",
    "                decel_val = 1 - (speed / prev_speed)\n",
    "                decel_str = f\"{decel_val:.2f}\"\n",
    "        \n",
    "        # Compute confidence\n",
    "        conf = 0.0\n",
    "        if reversal_val > 0:\n",
    "            conf += 0.3 * (0.5 + 0.5 * reversal_val)\n",
    "        if is_spike:\n",
    "            conf += 0.2 * min((speed / ref_speed) / 2.0, 1.5)\n",
    "        if decel_val > 0:\n",
    "            conf += 0.2 * decel_val\n",
    "        conf = min(conf, 0.7)\n",
    "        conf_str = f\"{conf:.2f}\" if conf > 0.1 else \"\"\n",
    "        \n",
    "        prev_vel = v\n",
    "    else:\n",
    "        speed_str = \"---\"\n",
    "        vx_str = \"---\"\n",
    "        vy_str = \"---\"\n",
    "        spike_str = \"\"\n",
    "        reversal_str = \"\"\n",
    "        decel_str = \"\"\n",
    "        conf_str = \"\"\n",
    "        conf = 0\n",
    "        is_spike = False\n",
    "    \n",
    "    # Notes\n",
    "    notes = []\n",
    "    if f == KNOWN_CONTACT_FRAME:\n",
    "        notes.append(\"<<< KNOWN CONTACT\")\n",
    "    if any(c['frame'] == f for c in contacts):\n",
    "        c = next(c for c in contacts if c['frame'] == f)\n",
    "        notes.append(f\"DETECTED (conf={c['confidence']:.2f})\")\n",
    "    \n",
    "    notes_str = \" | \".join(notes)\n",
    "    \n",
    "    # Highlight important rows\n",
    "    marker = \">>>\" if f == KNOWN_CONTACT_FRAME else \"   \"\n",
    "    \n",
    "    print(f\"{marker}{f:3d} {time_sec:7.2f}s {ball_str:>6} {speed_str:>10} {vx_str:>8} {vy_str:>8} {spike_str:>7} {reversal_str:>9} {decel_str:>7} {conf_str:>6}  {notes_str}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nKnown contact frame: {KNOWN_CONTACT_FRAME}\")\n",
    "print(f\"  Ball position at frame {KNOWN_CONTACT_FRAME}? {KNOWN_CONTACT_FRAME in ball_detections}\")\n",
    "print(f\"  Velocity data at frame {KNOWN_CONTACT_FRAME}? {KNOWN_CONTACT_FRAME in vel_dict}\")\n",
    "\n",
    "# Was it detected?\n",
    "detected_at_known = any(c['frame'] == KNOWN_CONTACT_FRAME for c in contacts)\n",
    "if detected_at_known:\n",
    "    print(f\"  Contact WAS detected at frame {KNOWN_CONTACT_FRAME}\")\n",
    "else:\n",
    "    # Find closest detection\n",
    "    if contacts:\n",
    "        closest = min(contacts, key=lambda c: abs(c['frame'] - KNOWN_CONTACT_FRAME))\n",
    "        print(f\"  Contact NOT detected at frame {KNOWN_CONTACT_FRAME}\")\n",
    "        print(f\"  Closest detection: frame {closest['frame']} ({abs(closest['frame'] - KNOWN_CONTACT_FRAME)} frames away)\")\n",
    "    else:\n",
    "        print(f\"  No contacts detected at all!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Analyze Selected Contact\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "\n",
    "from utils.coordinate_transforms import (\n",
    "    pelvis_origin_transform, estimate_ground_plane, apply_ground_plane\n",
    ")\n",
    "from src.pose_estimation import PoseEstimator\n",
    "from src.measurements import compute_measurements\n",
    "from src.visualization import (\n",
    "    draw_skeleton, draw_contact_point, save_annotated_frame\n",
    ")\n",
    "\n",
    "#@markdown ### Select which contact to analyze:\n",
    "CONTACT_INDEX = 0  #@param {type:\"integer\"}\n",
    "\n",
    "# Validate\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first to detect contacts!\")\n",
    "\n",
    "contacts = ANALYSIS_DATA['contacts']\n",
    "if len(contacts) == 0:\n",
    "    raise ValueError(\"No contacts were detected in the video.\")\n",
    "\n",
    "if CONTACT_INDEX < 0 or CONTACT_INDEX >= len(contacts):\n",
    "    raise ValueError(f\"Invalid contact index. Valid range: 0-{len(contacts)-1}\")\n",
    "\n",
    "contact = contacts[CONTACT_INDEX]\n",
    "frame_num = contact['frame']\n",
    "ball_pos = contact['ball_pos']\n",
    "ball_method = contact['ball_method']\n",
    "shot_type = ANALYSIS_DATA['shot_type']\n",
    "frames = ANALYSIS_DATA['frames']\n",
    "fps = ANALYSIS_DATA['fps']\n",
    "\n",
    "print(f\"Analyzing Contact {CONTACT_INDEX + 1}\")\n",
    "print(f\"  Frame: {frame_num} ({contact['time']:.2f}s)\")\n",
    "print(f\"  Detection confidence: {contact['confidence']:.0%}\")\n",
    "print(f\"  Ball position method: {ball_method}\")\n",
    "\n",
    "# Get frame\n",
    "frame = frames[frame_num]\n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "# Determine which wrist to use based on shot type\n",
    "if shot_type in [\"right_forehand\", \"right_backhand\"]:\n",
    "    contact_wrist_name = \"right_wrist\"\n",
    "else:\n",
    "    contact_wrist_name = \"left_wrist\"\n",
    "\n",
    "# --- Pose estimation ---\n",
    "print(\"\\nEstimating pose...\")\n",
    "pose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n",
    "landmarks, raw_result = pose_estimator.process_frame(frame)\n",
    "\n",
    "if landmarks is None:\n",
    "    pose_estimator.close()\n",
    "    raise ValueError(\"No pose detected in frame. The player may not be clearly visible.\")\n",
    "\n",
    "pixel_lm = pose_estimator.get_pixel_landmarks(raw_result, frame.shape)\n",
    "pose_estimator.close()\n",
    "print(\"  Pose detected successfully!\")\n",
    "\n",
    "# --- Get contact point (ball position or fallback to wrist) ---\n",
    "if ball_pos is not None:\n",
    "    contact_pixel = ball_pos\n",
    "    contact_source = f\"ball ({ball_method})\"\n",
    "    print(f\"  Using ball position as contact point: ({ball_pos[0]:.0f}, {ball_pos[1]:.0f})\")\n",
    "else:\n",
    "    if contact_wrist_name in pixel_lm:\n",
    "        contact_pixel = pixel_lm[contact_wrist_name]\n",
    "        contact_source = \"wrist (fallback)\"\n",
    "        print(f\"  Ball not detected - using wrist position as fallback\")\n",
    "    else:\n",
    "        raise ValueError(\"Neither ball nor wrist position available\")\n",
    "\n",
    "# --- Transform coordinates for measurements ---\n",
    "pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n",
    "centered = pelvis_origin_transform(landmarks)\n",
    "ground_z = estimate_ground_plane(centered)\n",
    "adjusted = apply_ground_plane(centered, ground_z)\n",
    "\n",
    "wrist_3d = landmarks.get(contact_wrist_name, np.zeros(3))\n",
    "contact_3d = wrist_3d.copy()\n",
    "\n",
    "if ball_pos is not None and contact_wrist_name in pixel_lm:\n",
    "    wrist_px = pixel_lm[contact_wrist_name]\n",
    "    px_offset_x = ball_pos[0] - wrist_px[0]\n",
    "    px_offset_y = ball_pos[1] - wrist_px[1]\n",
    "    scale = 0.001\n",
    "    contact_3d[0] += px_offset_x * scale\n",
    "    contact_3d[1] += px_offset_y * scale\n",
    "\n",
    "contact_adjusted = contact_3d - pelvis - np.array([0, 0, ground_z])\n",
    "\n",
    "# --- Compute measurements ---\n",
    "print(\"Computing measurements...\")\n",
    "meas = compute_measurements(adjusted, contact_adjusted)\n",
    "meas[\"contact_source\"] = contact_source\n",
    "meas[\"ball_detection_method\"] = ball_method\n",
    "meas[\"shot_type\"] = shot_type\n",
    "meas[\"frame_num\"] = frame_num\n",
    "meas[\"contact_confidence\"] = contact['confidence']\n",
    "\n",
    "# --- Create output ---\n",
    "output_dir = \"/content/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTACT FRAME ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "annotated = frame.copy()\n",
    "annotated = draw_skeleton(annotated, pixel_lm, thickness=3)\n",
    "cx, cy = int(contact_pixel[0]), int(contact_pixel[1])\n",
    "draw_contact_point(annotated, cx, cy, radius=15)\n",
    "\n",
    "label = f\"CONTACT ({contact_source})\"\n",
    "cv2.putText(annotated, label, (cx + 20, cy - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "info_text = f\"Frame {frame_num} | {contact['time']:.2f}s | Conf: {contact['confidence']:.0%}\"\n",
    "cv2.putText(annotated, info_text, (20, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "out_path = os.path.join(output_dir, f\"contact_{CONTACT_INDEX+1}_frame_{frame_num}.png\")\n",
    "save_annotated_frame(annotated, out_path)\n",
    "\n",
    "print(\"\\nAnnotated contact frame:\")\n",
    "display(IPImage(filename=out_path, width=800))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTACT POINT MEASUREMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShot type: {shot_type}\")\n",
    "print(f\"Contact source: {contact_source}\")\n",
    "print()\n",
    "print(f\"Lateral offset:      {meas.get('lateral_offset_cm', 0):>7.1f} cm\")\n",
    "print(f\"Forward/back:        {meas.get('forward_back_cm', 0):>7.1f} cm\")\n",
    "print(f\"Height above ground: {meas.get('height_above_ground_cm', 0):>7.1f} cm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"measurements_contact_{CONTACT_INDEX+1}.csv\")\n",
    "pd.DataFrame([meas]).to_csv(csv_path, index=False)\n",
    "print(f\"\\nMeasurements saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Batch Analysis - Analyze All Contacts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils.coordinate_transforms import (\n",
    "    pelvis_origin_transform, estimate_ground_plane, apply_ground_plane\n",
    ")\n",
    "from src.pose_estimation import PoseEstimator\n",
    "from src.measurements import compute_measurements\n",
    "\n",
    "if 'ANALYSIS_DATA' not in dir():\n",
    "    raise ValueError(\"Please run cell 2 first!\")\n",
    "\n",
    "contacts = ANALYSIS_DATA['contacts']\n",
    "frames = ANALYSIS_DATA['frames']\n",
    "fps = ANALYSIS_DATA['fps']\n",
    "shot_type = ANALYSIS_DATA['shot_type']\n",
    "ball_detections = ANALYSIS_DATA['ball_detections']\n",
    "\n",
    "if len(contacts) == 0:\n",
    "    print(\"No contacts to analyze.\")\n",
    "else:\n",
    "    print(f\"Analyzing {len(contacts)} contacts...\\n\")\n",
    "    \n",
    "    all_measurements = []\n",
    "    pose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n",
    "    \n",
    "    if shot_type in [\"right_forehand\", \"right_backhand\"]:\n",
    "        contact_wrist_name = \"right_wrist\"\n",
    "    else:\n",
    "        contact_wrist_name = \"left_wrist\"\n",
    "    \n",
    "    for contact in contacts:\n",
    "        idx = contact['index']\n",
    "        frame_num = contact['frame']\n",
    "        ball_pos = contact['ball_pos']\n",
    "        ball_method = contact['ball_method']\n",
    "        \n",
    "        print(f\"Contact {idx+1}: Frame {frame_num}...\", end=\" \")\n",
    "        \n",
    "        frame = frames[frame_num]\n",
    "        landmarks, raw_result = pose_estimator.process_frame(frame)\n",
    "        \n",
    "        if landmarks is None:\n",
    "            print(\"SKIPPED (no pose)\")\n",
    "            continue\n",
    "        \n",
    "        pixel_lm = pose_estimator.get_pixel_landmarks(raw_result, frame.shape)\n",
    "        \n",
    "        if ball_pos is not None:\n",
    "            contact_source = f\"ball ({ball_method})\"\n",
    "        elif contact_wrist_name in pixel_lm:\n",
    "            ball_pos = pixel_lm[contact_wrist_name]\n",
    "            contact_source = \"wrist (fallback)\"\n",
    "        else:\n",
    "            print(\"SKIPPED (no contact point)\")\n",
    "            continue\n",
    "        \n",
    "        pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n",
    "        centered = pelvis_origin_transform(landmarks)\n",
    "        ground_z = estimate_ground_plane(centered)\n",
    "        adjusted = apply_ground_plane(centered, ground_z)\n",
    "        \n",
    "        wrist_3d = landmarks.get(contact_wrist_name, np.zeros(3))\n",
    "        contact_3d = wrist_3d.copy()\n",
    "        contact_adjusted = contact_3d - pelvis - np.array([0, 0, ground_z])\n",
    "        \n",
    "        meas = compute_measurements(adjusted, contact_adjusted)\n",
    "        meas[\"contact_index\"] = idx + 1\n",
    "        meas[\"frame_num\"] = frame_num\n",
    "        meas[\"time_sec\"] = contact['time']\n",
    "        meas[\"contact_confidence\"] = contact['confidence']\n",
    "        meas[\"contact_source\"] = contact_source\n",
    "        \n",
    "        all_measurements.append(meas)\n",
    "        print(\"OK\")\n",
    "    \n",
    "    pose_estimator.close()\n",
    "    \n",
    "    if all_measurements:\n",
    "        output_dir = \"/content/output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df = pd.DataFrame(all_measurements)\n",
    "        csv_path = os.path.join(output_dir, \"all_contacts_measurements.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        display(df[['contact_index', 'frame_num', 'time_sec', 'contact_confidence',\n",
    "                    'lateral_offset_cm', 'forward_back_cm', 'height_above_ground_cm']])\n",
    "        print(f\"\\nSaved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Download All Results\n",
    "from google.colab import files as colab_files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/output\"\n",
    "all_files = glob.glob(os.path.join(output_dir, \"*\"))\n",
    "\n",
    "print(\"Files available:\")\n",
    "for f in all_files:\n",
    "    size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "    print(f\"  {os.path.basename(f)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"\\nDownloading...\")\n",
    "for f in all_files:\n",
    "    colab_files.download(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}