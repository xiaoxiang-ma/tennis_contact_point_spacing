{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis Contact Point Analysis - MVP\n",
    "\n",
    "Upload a tennis rally video to detect contact points, estimate 3D pose, and measure contact position relative to body landmarks.\n",
    "\n",
    "**Steps:**\n",
    "1. Run the **Setup** cell to install dependencies\n",
    "2. Run the **Upload & Process** cell to upload your video and run the pipeline\n",
    "3. View results and download annotated images + CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup - Install dependencies and clone repo\n",
    "import os\n",
    "\n",
    "# Clone the repository if not already present\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/tennis_contact_point_spacing.git\"  # Update this\n",
    "REPO_DIR = \"/content/tennis_contact_point_spacing\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "!pip install -q -r {REPO_DIR}/requirements.txt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Upload & Process Video\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from google.colab import files\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "\n",
    "from utils.video_io import load_video\n",
    "from utils.coordinate_transforms import (\n",
    "    pelvis_origin_transform, estimate_ground_plane,\n",
    "    apply_ground_plane, estimate_player_height_scale\n",
    ")\n",
    "from src.ball_detection import BallTracker\n",
    "from src.pose_estimation import PoseEstimator, LANDMARK_MAP\n",
    "from src.contact_detection import detect_contacts\n",
    "from src.measurements import compute_measurements\n",
    "from src.visualization import (\n",
    "    annotate_contact_frame, save_annotated_frame, _world_to_pixel\n",
    ")\n",
    "\n",
    "# --- Upload video ---\n",
    "print(\"Upload your tennis video (MP4, MOV, AVI):\")\n",
    "uploaded = files.upload()\n",
    "video_filename = list(uploaded.keys())[0]\n",
    "video_path = os.path.join(\"/content\", video_filename)\n",
    "with open(video_path, \"wb\") as f:\n",
    "    f.write(uploaded[video_filename])\n",
    "\n",
    "# --- Load video ---\n",
    "print(f\"\\nLoading video: {video_filename}\")\n",
    "frames, meta = load_video(video_path)\n",
    "fps = meta[\"fps\"]\n",
    "print(f\"  {len(frames)} frames, {meta['width']}x{meta['height']}, {fps:.1f} fps, {meta['duration_sec']:.1f}s\")\n",
    "\n",
    "# --- Ball detection ---\n",
    "print(\"\\nDetecting ball...\")\n",
    "tracker = BallTracker()  # HSV fallback (no TrackNet weights)\n",
    "ball_detections = tracker.detect_all(frames, progress=True)\n",
    "print(f\"  Ball detected in {len(ball_detections)}/{len(frames)} frames\")\n",
    "\n",
    "# --- Pose estimation (run on all frames to get wrist positions) ---\n",
    "print(\"\\nEstimating pose...\")\n",
    "pose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n",
    "\n",
    "# We need image-space landmarks for visualization, so use raw MediaPipe\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=True, model_complexity=2)\n",
    "\n",
    "# Get wrist positions for contact detection (in pixel coords)\n",
    "wrist_positions = {}  # frame_num -> (x, y)\n",
    "pose_cache = {}  # frame_num -> (world_landmarks, mp_result)\n",
    "\n",
    "# Only process frames near ball detections + some padding\n",
    "ball_frames = set(d[0] for d in ball_detections)\n",
    "frames_to_process = set()\n",
    "for bf in ball_frames:\n",
    "    for offset in range(-5, 6):\n",
    "        f = bf + offset\n",
    "        if 0 <= f < len(frames):\n",
    "            frames_to_process.add(f)\n",
    "\n",
    "for i in tqdm(sorted(frames_to_process), desc=\"Pose estimation\"):\n",
    "    rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)\n",
    "    result = mp_pose.process(rgb)\n",
    "    if result.pose_world_landmarks is not None:\n",
    "        # Extract world landmarks\n",
    "        landmarks = {}\n",
    "        wl = result.pose_world_landmarks.landmark\n",
    "        for name, idx in LANDMARK_MAP.items():\n",
    "            lm = wl[idx]\n",
    "            landmarks[name] = np.array([lm.x, lm.y, lm.z])\n",
    "        if \"left_hip\" in landmarks and \"right_hip\" in landmarks:\n",
    "            landmarks[\"pelvis\"] = (landmarks[\"left_hip\"] + landmarks[\"right_hip\"]) / 2.0\n",
    "        if \"nose\" in landmarks:\n",
    "            landmarks[\"head\"] = landmarks[\"nose\"]\n",
    "        pose_cache[i] = (landmarks, result)\n",
    "\n",
    "        # Wrist pixel positions (use right wrist as dominant hand guess)\n",
    "        if result.pose_landmarks is not None:\n",
    "            h, w = frames[i].shape[:2]\n",
    "            rw = result.pose_landmarks.landmark[16]  # right wrist\n",
    "            lw = result.pose_landmarks.landmark[15]  # left wrist\n",
    "            # Use the wrist that's further from the body center (more extended)\n",
    "            rw_x, rw_y = rw.x * w, rw.y * h\n",
    "            lw_x, lw_y = lw.x * w, lw.y * h\n",
    "            # Pick the wrist further from body midpoint\n",
    "            mid_x = w / 2\n",
    "            if abs(rw_x - mid_x) > abs(lw_x - mid_x):\n",
    "                wrist_positions[i] = (rw_x, rw_y)\n",
    "            else:\n",
    "                wrist_positions[i] = (lw_x, lw_y)\n",
    "\n",
    "mp_pose.close()\n",
    "print(f\"  Pose estimated for {len(pose_cache)} frames\")\n",
    "\n",
    "# --- Contact detection ---\n",
    "print(\"\\nDetecting contacts...\")\n",
    "contacts = detect_contacts(\n",
    "    ball_detections, fps,\n",
    "    wrist_positions=wrist_positions,\n",
    "    velocity_spike_threshold=1.8,\n",
    "    wrist_proximity_px=200,\n",
    "    min_frame_gap=int(fps * 0.5)  # at least 0.5s apart\n",
    ")\n",
    "print(f\"  Found {len(contacts)} contact(s)\")\n",
    "\n",
    "if not contacts:\n",
    "    print(\"\\nNo contacts detected. Try adjusting camera angle or lighting.\")\n",
    "\n",
    "# --- Process each contact ---\n",
    "output_dir = \"/content/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "all_measurements = []\n",
    "\n",
    "for contact_frame, confidence in contacts:\n",
    "    if contact_frame not in pose_cache:\n",
    "        # Try to get pose for this specific frame\n",
    "        rgb = cv2.cvtColor(frames[contact_frame], cv2.COLOR_BGR2RGB)\n",
    "        result = mp.solutions.pose.Pose(static_image_mode=True, model_complexity=2).process(rgb)\n",
    "        if result.pose_world_landmarks is None:\n",
    "            print(f\"  Frame {contact_frame}: no pose detected, skipping\")\n",
    "            continue\n",
    "        landmarks = {}\n",
    "        wl = result.pose_world_landmarks.landmark\n",
    "        for name, idx in LANDMARK_MAP.items():\n",
    "            lm = wl[idx]\n",
    "            landmarks[name] = np.array([lm.x, lm.y, lm.z])\n",
    "        if \"left_hip\" in landmarks and \"right_hip\" in landmarks:\n",
    "            landmarks[\"pelvis\"] = (landmarks[\"left_hip\"] + landmarks[\"right_hip\"]) / 2.0\n",
    "        if \"nose\" in landmarks:\n",
    "            landmarks[\"head\"] = landmarks[\"nose\"]\n",
    "        pose_cache[contact_frame] = (landmarks, result)\n",
    "\n",
    "    landmarks, mp_result = pose_cache[contact_frame]\n",
    "\n",
    "    # Determine dominant wrist (the one further extended)\n",
    "    lw = landmarks.get(\"left_wrist\", np.zeros(3))\n",
    "    rw = landmarks.get(\"right_wrist\", np.zeros(3))\n",
    "    pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n",
    "    if np.linalg.norm(rw - pelvis) > np.linalg.norm(lw - pelvis):\n",
    "        contact_wrist_name = \"right_wrist\"\n",
    "        contact_point = rw\n",
    "    else:\n",
    "        contact_wrist_name = \"left_wrist\"\n",
    "        contact_point = lw\n",
    "\n",
    "    # Transform to pelvis-centered, ground-adjusted coordinates\n",
    "    centered = pelvis_origin_transform(landmarks)\n",
    "    ground_z = estimate_ground_plane(centered)\n",
    "    adjusted = apply_ground_plane(centered, ground_z)\n",
    "\n",
    "    # Contact point in same coordinate system\n",
    "    contact_adjusted = contact_point - pelvis - np.array([0, 0, ground_z])\n",
    "\n",
    "    # Compute measurements\n",
    "    meas = compute_measurements(adjusted, contact_adjusted)\n",
    "    meas[\"frame_number\"] = contact_frame\n",
    "    meas[\"timestamp\"] = contact_frame / fps\n",
    "    meas[\"confidence\"] = confidence\n",
    "    meas[\"contact_x\"] = contact_point[0]\n",
    "    meas[\"contact_y\"] = contact_point[1]\n",
    "    meas[\"contact_z\"] = contact_point[2]\n",
    "    all_measurements.append(meas)\n",
    "\n",
    "    # Visualize\n",
    "    pixel_lm = _world_to_pixel(landmarks, frames[contact_frame], mp_result)\n",
    "    annotated = annotate_contact_frame(\n",
    "        frames[contact_frame], pixel_lm, contact_wrist_name,\n",
    "        meas, contact_frame, fps\n",
    "    )\n",
    "    out_path = os.path.join(output_dir, f\"contact_frame_{contact_frame}.png\")\n",
    "    save_annotated_frame(annotated, out_path)\n",
    "    print(f\"  Contact at frame {contact_frame} (t={contact_frame/fps:.2f}s, conf={confidence:.2f})\")\n",
    "\n",
    "# Save CSV\n",
    "if all_measurements:\n",
    "    video_stem = os.path.splitext(video_filename)[0]\n",
    "    csv_path = os.path.join(output_dir, f\"measurements_{video_stem}.csv\")\n",
    "    df = pd.DataFrame(all_measurements)\n",
    "    # Reorder columns\n",
    "    first_cols = [\"frame_number\", \"timestamp\", \"confidence\",\n",
    "                  \"contact_x\", \"contact_y\", \"contact_z\"]\n",
    "    other_cols = [c for c in df.columns if c not in first_cols]\n",
    "    df = df[first_cols + sorted(other_cols)]\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nMeasurements saved to {csv_path}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Results - View annotated frames & download\n",
    "import glob\n",
    "from IPython.display import display, Image as IPImage, HTML\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = \"/content/output\"\n",
    "\n",
    "# Display annotated images\n",
    "png_files = sorted(glob.glob(os.path.join(output_dir, \"contact_frame_*.png\")))\n",
    "if png_files:\n",
    "    print(f\"Found {len(png_files)} contact frame(s):\\n\")\n",
    "    for png_path in png_files:\n",
    "        print(os.path.basename(png_path))\n",
    "        display(IPImage(filename=png_path, width=800))\n",
    "        print()\n",
    "else:\n",
    "    print(\"No annotated frames found.\")\n",
    "\n",
    "# Display measurements table\n",
    "csv_files = glob.glob(os.path.join(output_dir, \"measurements_*.csv\"))\n",
    "if csv_files:\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    print(\"\\nMeasurements:\")\n",
    "    display(df)\n",
    "\n",
    "# Download links\n",
    "print(\"\\n--- Download Files ---\")\n",
    "from google.colab import files as colab_files\n",
    "for f in png_files + csv_files:\n",
    "    colab_files.download(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
