{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tennis Contact Point Detection System\n\nA two-stage detection pipeline for analyzing tennis ball-racket contact in stroke videos.\n\n## Pipeline Overview\n\n### Stage 1: Temporal Detection (WHEN)\nDetermines the **frame number** where contact occurs.\n\n**Primary**: Audio-based contact detection\n- Transient peak analysis with bandpass filtering (1-4kHz)\n- Isolates ball-string impact sound from background noise\n\n**Secondary**: Visual verification\n- Ball trajectory change detection (velocity reversal, speed spike, deceleration)\n- Trajectory analysis from TrackNet ball tracking\n\n**Output**: Contact frame number with confidence score\n- HIGH confidence: Audio + visual signals agree\n- MEDIUM confidence: Audio only\n- LOW confidence (<70%): Manual selection fallback available\n\n### Stage 2: Spatial Localization (WHERE)\nDetermines the **3D position** of contact relative to the player's body.\n\n**Process**:\n1. MediaPipe Pose extracts body keypoints (wrist, elbow, shoulder, hip, ankle)\n2. CV techniques pinpoint contact location (ball position, racket center estimation)\n3. Transform to body-relative coordinate system\n\n**Output**:\n- 3D contact coordinates (pelvis-centered)\n- Continuous measurements: height (cm), forward/back (cm), lateral (cm)\n- Categorical labels: low/hip-level/waist-level/chest-level/high, behind/neutral/forward\n- Position relative to shoulder and hip landmarks\n\n---\n\n## Cells\n\n1. **Setup** - Install dependencies\n2. **Upload & Stage 1 Detection** - Temporal contact detection (audio + visual)\n3. **Review Detections** - Diagnostic video and confidence check\n4. **Manual Frame Selection** - Fallback for low confidence contacts\n5. **Stage 2: Spatial Localization** - Analyze selected contact\n6. **Batch Analysis** - Analyze all contacts with full measurements\n7. **Download Results**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1. Setup - Install Dependencies\nimport os, sys, shutil\n\nREPO_URL = \"https://github.com/xiaoxiang-ma/tennis_contact_point_spacing.git\"\nREPO_DIR = \"/content/tennis_contact_point_spacing\"\n\n# Always re-clone to ensure latest code\nif os.path.exists(REPO_DIR):\n    shutil.rmtree(REPO_DIR)\n!git clone {REPO_URL} {REPO_DIR}\n\n!pip install -q -r {REPO_DIR}/requirements.txt\n\n# Install ipywidgets for manual selection interface\n!pip install -q ipywidgets\n\n# Clear any cached module imports from previous runs\nfor mod_name in list(sys.modules.keys()):\n    if mod_name.startswith((\"src.\", \"utils.\")):\n        del sys.modules[mod_name]\n\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\n\n# Create weights directory\nos.makedirs(os.path.join(REPO_DIR, \"weights\"), exist_ok=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETUP COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nTwo-Stage Contact Detection Pipeline Ready:\")\nprint(\"  Stage 1: Temporal Detection (audio + visual)\")\nprint(\"  Stage 2: Spatial Localization (pose + CV)\")\nprint(\"\\nTrackNet weights will download automatically on first use (~50MB)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Upload Video & Stage 1: Temporal Detection\nimport numpy as np\nimport cv2\nfrom google.colab import files\nfrom IPython.display import display, Image as IPImage, HTML\nimport os\n\nfrom utils.video_io import load_video\nfrom src.tracknet import TrackNetDetector, TrackNetV4Detector\nfrom src.contact_detection import detect_contacts, get_contact_ball_position\n\n#@markdown ## Shot Settings\nSHOT_TYPE = \"right_forehand\"  #@param [\"right_forehand\", \"right_backhand\", \"left_forehand\", \"left_backhand\"]\n\n#@markdown ## Stage 1: Temporal Detection Settings\n#@markdown ---\n#@markdown ### Primary: Audio Detection\n#@markdown Audio-based detection uses transient peak analysis with bandpass filtering (1-4kHz)\nUSE_AUDIO = True  #@param {type:\"boolean\"}\n\n#@markdown ### Secondary: Visual Detection (TrackNet)\n#@markdown Ball trajectory analysis for velocity reversal/spike detection\nCONFIDENCE_THRESHOLD = 0.3  #@param {type:\"slider\", min:0.1, max:0.9, step:0.1}\nFILL_GAPS = True  #@param {type:\"boolean\"}\nMAX_GAP_FRAMES = 10  #@param {type:\"slider\", min:3, max:20, step:1}\n\n#@markdown ### Motion Attention (TrackNetV4)\nUSE_MOTION_ATTENTION = True  #@param {type:\"boolean\"}\nMOTION_BOOST = 2.0  #@param {type:\"slider\", min:1.0, max:4.0, step:0.5}\nMOTION_MIN_ATTENTION = 0.3  #@param {type:\"slider\", min:0.1, max:0.5, step:0.1}\n\n#@markdown ### Debug Options\nDEBUG_MODE = True  #@param {type:\"boolean\"}\nSAVE_DEBUG_FRAMES = False  #@param {type:\"boolean\"}\n\n# Create output directory\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Upload video ---\nprint(\"=\"*60)\nprint(\"STAGE 1: TEMPORAL DETECTION (When does contact occur?)\")\nprint(\"=\"*60)\nprint(\"\\nUpload your tennis video (MP4, MOV, etc.):\")\nuploaded = files.upload()\nvideo_filename = list(uploaded.keys())[0]\nvideo_path = os.path.join(\"/content\", video_filename)\nwith open(video_path, \"wb\") as f:\n    f.write(uploaded[video_filename])\n\n# --- Load video ---\nprint(f\"\\nLoading video: {video_filename}\")\nframes, metadata = load_video(video_path)\nfps = metadata[\"fps\"]\nprint(f\"  Resolution: {metadata['width']}x{metadata['height']}\")\nprint(f\"  Frame rate: {fps:.1f} fps\")\nprint(f\"  Duration: {metadata['duration_sec']:.2f}s ({len(frames)} frames)\")\n\n# --- Initialize TrackNet ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Initializing ball tracking (TrackNet)...\")\nif USE_MOTION_ATTENTION:\n    print(f\"  Using TrackNetV4 with motion attention\")\n    print(f\"  Motion boost: {MOTION_BOOST}x, Min attention: {MOTION_MIN_ATTENTION}\")\n    tracknet = TrackNetV4Detector(\n        weights_path=None,\n        device=None,\n        confidence_threshold=CONFIDENCE_THRESHOLD,\n        save_debug_frames=SAVE_DEBUG_FRAMES,\n        debug_output_dir=os.path.join(output_dir, \"tracknet_debug\"),\n        motion_boost=MOTION_BOOST,\n        motion_min_attention=MOTION_MIN_ATTENTION,\n        use_motion_attention=True,\n    )\nelse:\n    print(f\"  Using TrackNet (standard)\")\n    tracknet = TrackNetDetector(\n        weights_path=None,\n        device=None,\n        confidence_threshold=CONFIDENCE_THRESHOLD,\n        save_debug_frames=SAVE_DEBUG_FRAMES,\n        debug_output_dir=os.path.join(output_dir, \"tracknet_debug\"),\n    )\n\nif not tracknet.weights_loaded:\n    print(\"\\n⚠️  WARNING: TrackNet weights failed to load!\")\n    print(\"    Ball detection may not work properly.\")\n\n# --- Run Stage 1: Temporal Detection ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Running temporal detection...\")\nprint(\"  Primary: Audio analysis (1-4kHz transient detection)\")\nprint(\"  Secondary: Trajectory analysis (velocity reversal/spike)\")\nif FILL_GAPS:\n    print(f\"  Gap filling: enabled (max {MAX_GAP_FRAMES} frames)\")\n\ncontacts, ball_detections = detect_contacts(\n    video_path=video_path,\n    frames=frames,\n    fps=fps,\n    tracknet_detector=tracknet,\n    use_audio=USE_AUDIO,\n    fill_gaps=FILL_GAPS,\n    max_gap_frames=MAX_GAP_FRAMES,\n    debug=DEBUG_MODE,\n    save_debug_frames=SAVE_DEBUG_FRAMES,\n)\n\n# Build trajectory for diagnostics\ntrajectory = tracknet.get_ball_trajectory(ball_detections)\n\n# --- Display Stage 1 Results ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"STAGE 1 RESULTS: TEMPORAL DETECTION\")\nprint(\"=\"*60)\nprint(f\"\\nBall tracking: {len(ball_detections)}/{len(frames)} frames ({100*len(ball_detections)/len(frames):.1f}%)\")\n\nprint(f\"\\nDetected {len(contacts)} contact(s):\")\nprint(\"-\"*60)\n\n# Confidence threshold for manual selection recommendation\nLOW_CONFIDENCE_THRESHOLD = 0.7\nneeds_manual = False\n\ncontact_info = []\nfor i, (frame_num, confidence, source) in enumerate(contacts):\n    time_sec = frame_num / fps\n    ball_pos, ball_method = get_contact_ball_position(frame_num, ball_detections)\n\n    # Determine confidence level\n    if source == 'both':\n        conf_level = \"HIGH (audio+visual)\"\n    elif confidence >= LOW_CONFIDENCE_THRESHOLD:\n        conf_level = \"MEDIUM\"\n    else:\n        conf_level = \"LOW ⚠️\"\n        needs_manual = True\n\n    contact_info.append({\n        'index': i,\n        'frame': frame_num,\n        'time': time_sec,\n        'confidence': confidence,\n        'confidence_level': conf_level,\n        'source': source,\n        'ball_pos': ball_pos,\n        'ball_method': ball_method,\n    })\n\n    print(f\"\\n  Contact {i+1}: Frame {frame_num} ({time_sec:.2f}s)\")\n    print(f\"    Confidence: {confidence:.0%} - {conf_level}\")\n    print(f\"    Source: {source}\")\n    if ball_pos:\n        print(f\"    Ball position: ({ball_pos[0]:.0f}, {ball_pos[1]:.0f}) [{ball_method}]\")\n\nif not contacts:\n    print(\"\\n  ⚠️  No contacts detected automatically.\")\n    print(\"      Please use manual frame selection (Cell 4)\")\n    needs_manual = True\n\n# Store for subsequent cells\nANALYSIS_DATA = {\n    'frames': frames,\n    'fps': fps,\n    'metadata': metadata,\n    'contacts': contact_info,\n    'contacts_raw': contacts,\n    'ball_detections': ball_detections,\n    'trajectory': trajectory,\n    'shot_type': SHOT_TYPE,\n    'video_path': video_path,\n    'motion_attention_enabled': USE_MOTION_ATTENTION,\n    'needs_manual_selection': needs_manual,\n}\n\nprint(\"\\n\" + \"=\"*60)\nif needs_manual:\n    print(\"⚠️  LOW CONFIDENCE DETECTED - Manual review recommended\")\n    print(\"   Run Cell 3 for diagnostic video, then Cell 4 for manual selection\")\nelse:\n    print(\"✓ Detection complete - Run Cell 3 to verify with diagnostic video\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Review Detections - Diagnostic Video\nimport os\nfrom IPython.display import display, HTML\nfrom google.colab import files as colab_files\n\nfrom src.visualization import create_diagnostic_video\n\n#@markdown ### Diagnostic Settings\n#@markdown Set a known real contact frame (if you know it) to compare with detection:\nKNOWN_CONTACT_FRAME = 0  #@param {type:\"integer\"}\nSHOW_TRAJECTORY_TRAIL = True  #@param {type:\"boolean\"}\nTRAJECTORY_TAIL_FRAMES = 30  #@param {type:\"integer\"}\n\nif 'ANALYSIS_DATA' not in dir():\n    raise ValueError(\"Please run Cell 2 first!\")\n\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\noutput_path = os.path.join(output_dir, \"diagnostic_video.mp4\")\n\nprint(\"=\"*60)\nprint(\"DETECTION VERIFICATION\")\nprint(\"=\"*60)\nprint(\"\\nGenerating diagnostic video with overlays...\")\nprint(\"  - Ball position (green circle)\")\nprint(\"  - Velocity vector (yellow arrow)\")\nprint(\"  - Detection signals: SPIKE (cyan), REVERSAL (magenta), DECEL (orange)\")\nprint(\"  - Detected contacts (red border)\")\nif KNOWN_CONTACT_FRAME > 0:\n    print(f\"  - Known contact frame {KNOWN_CONTACT_FRAME} (yellow border)\")\n\ncreate_diagnostic_video(\n    frames=ANALYSIS_DATA['frames'],\n    ball_detections=ANALYSIS_DATA['ball_detections'],\n    contacts=ANALYSIS_DATA['contacts_raw'],\n    fps=ANALYSIS_DATA['fps'],\n    output_path=output_path,\n    known_contact_frame=KNOWN_CONTACT_FRAME if KNOWN_CONTACT_FRAME > 0 else None,\n    show_trajectory=SHOW_TRAJECTORY_TRAIL,\n    trajectory_tail=TRAJECTORY_TAIL_FRAMES,\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VIDEO OVERLAY LEGEND\")\nprint(\"=\"*60)\nprint(\"\"\"\nTOP-LEFT PANEL:\n  Frame/Time     - Current frame number and timestamp\n  Ball           - DETECTED (green) or MISSING (red)\n  Speed          - Current ball speed in px/s\n  Signal boxes   - SPIKE (cyan), REVERSAL (magenta), DECEL (orange)\n  Conf bar       - Detection confidence (green=high, yellow=medium, orange=low)\n\nON FRAME:\n  Green circle   - Ball position (size = confidence)\n  Yellow arrow   - Velocity direction\n  Green trail    - Recent ball trajectory\n  RED border     - Frame detected as contact\n  YELLOW border  - Known real contact frame (if set)\n\"\"\")\n\n# Display video\nfrom base64 import b64encode\nvideo_data = open(output_path, 'rb').read()\nvideo_b64 = b64encode(video_data).decode()\ndisplay(HTML(f'''\n<video width=\"800\" controls>\n  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\">\n</video>\n'''))\n\n# Show detection summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETECTION SUMMARY\")\nprint(\"=\"*60)\ncontacts = ANALYSIS_DATA['contacts']\nif contacts:\n    for c in contacts:\n        print(f\"\\n  Contact {c['index']+1}: Frame {c['frame']} ({c['time']:.2f}s)\")\n        print(f\"    Confidence: {c['confidence']:.0%} - {c['confidence_level']}\")\n        print(f\"    Source: {c['source']}\")\nelse:\n    print(\"\\n  No contacts detected.\")\n\n# Recommendation\nprint(\"\\n\" + \"-\"*60)\nif ANALYSIS_DATA.get('needs_manual_selection', False):\n    print(\"⚠️  RECOMMENDATION: Use Cell 4 for manual frame selection\")\n    print(\"   Some detections have low confidence (<70%)\")\nelse:\n    print(\"✓ Detections look good - proceed to Stage 2 (Cell 5)\")\n\nprint(f\"\\nVideo saved to: {output_path}\")\nprint(\"Downloading...\")\ncolab_files.download(output_path)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4. Manual Frame Selection (Fallback for Low Confidence)\nimport numpy as np\nimport cv2\nfrom IPython.display import display, HTML, clear_output\nimport ipywidgets as widgets\nfrom base64 import b64encode\nimport os\n\n#@markdown ### Manual Selection Options\n#@markdown Use this cell when automatic detection has low confidence (<70%)\n#@markdown or when no contacts were detected.\n\n#@markdown Enter the contact frame number manually:\nMANUAL_CONTACT_FRAME = 0  #@param {type:\"integer\"}\n\n#@markdown Or use the interactive selector below:\nUSE_INTERACTIVE_SELECTOR = True  #@param {type:\"boolean\"}\n\nif 'ANALYSIS_DATA' not in dir():\n    raise ValueError(\"Please run Cell 2 first!\")\n\nframes = ANALYSIS_DATA['frames']\nfps = ANALYSIS_DATA['fps']\ncontacts = ANALYSIS_DATA['contacts']\nball_detections = ANALYSIS_DATA['ball_detections']\n\nprint(\"=\"*60)\nprint(\"MANUAL FRAME SELECTION\")\nprint(\"=\"*60)\n\nif contacts:\n    print(\"\\nCurrently detected contacts:\")\n    for c in contacts:\n        status = \"✓\" if c['confidence'] >= 0.7 else \"⚠️ LOW\"\n        print(f\"  {status} Contact {c['index']+1}: Frame {c['frame']} ({c['time']:.2f}s) - {c['confidence']:.0%}\")\nelse:\n    print(\"\\nNo contacts were automatically detected.\")\n\nprint(\"\\n\" + \"-\"*60)\n\nif MANUAL_CONTACT_FRAME > 0:\n    # Use the manually specified frame\n    manual_frame = MANUAL_CONTACT_FRAME\n    print(f\"\\nUsing manually specified frame: {manual_frame}\")\n\n    # Get ball position at this frame\n    from src.contact_detection import get_contact_ball_position\n    ball_pos, ball_method = get_contact_ball_position(manual_frame, ball_detections)\n\n    # Add or replace contact\n    new_contact = {\n        'index': 0,\n        'frame': manual_frame,\n        'time': manual_frame / fps,\n        'confidence': 1.0,\n        'confidence_level': 'MANUAL',\n        'source': 'manual',\n        'ball_pos': ball_pos,\n        'ball_method': ball_method,\n    }\n\n    # Update ANALYSIS_DATA\n    ANALYSIS_DATA['contacts'] = [new_contact]\n    ANALYSIS_DATA['contacts_raw'] = [(manual_frame, 1.0, 'manual')]\n    ANALYSIS_DATA['needs_manual_selection'] = False\n\n    print(f\"  Time: {manual_frame / fps:.2f}s\")\n    if ball_pos:\n        print(f\"  Ball position: ({ball_pos[0]:.0f}, {ball_pos[1]:.0f}) [{ball_method}]\")\n\n    # Show the frame\n    frame = frames[manual_frame].copy()\n    h, w = frame.shape[:2]\n    cv2.putText(frame, f\"MANUAL CONTACT - Frame {manual_frame}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3)\n    cv2.putText(frame, f\"MANUAL CONTACT - Frame {manual_frame}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n    if ball_pos:\n        cv2.circle(frame, (int(ball_pos[0]), int(ball_pos[1])), 15, (0, 255, 0), 2)\n        cv2.circle(frame, (int(ball_pos[0]), int(ball_pos[1])), 5, (0, 255, 0), -1)\n\n    _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])\n    b64 = b64encode(buffer).decode()\n    display(HTML(f'<img src=\"data:image/jpeg;base64,{b64}\" width=\"{min(w, 800)}\"/>'))\n\n    print(\"\\n✓ Manual contact frame set. Proceed to Stage 2 (Cell 5)\")\n\nelif USE_INTERACTIVE_SELECTOR:\n    # Interactive frame browser\n    print(\"\\nInteractive Frame Browser\")\n    print(\"Use the slider to find the exact contact frame.\")\n    print(\"-\"*60)\n\n    # State for selected frame\n    selected_frame = {'value': contacts[0]['frame'] if contacts else len(frames) // 2}\n\n    # Create widgets\n    frame_slider = widgets.IntSlider(\n        value=selected_frame['value'],\n        min=0,\n        max=len(frames) - 1,\n        step=1,\n        description='Frame:',\n        continuous_update=False,\n        layout=widgets.Layout(width='80%'),\n    )\n\n    step_buttons = widgets.HBox([\n        widgets.Button(description='<< -10', layout=widgets.Layout(width='80px')),\n        widgets.Button(description='< -1', layout=widgets.Layout(width='70px')),\n        widgets.Button(description='+1 >', layout=widgets.Layout(width='70px')),\n        widgets.Button(description='+10 >>', layout=widgets.Layout(width='80px')),\n    ])\n\n    select_button = widgets.Button(\n        description='✓ Select This Frame as Contact',\n        button_style='success',\n        layout=widgets.Layout(width='300px'),\n    )\n\n    output = widgets.Output()\n\n    def update_display(frame_num):\n        frame = frames[frame_num].copy()\n        h, w = frame.shape[:2]\n        time_sec = frame_num / fps\n\n        # Add frame info\n        info_text = f\"Frame {frame_num}/{len(frames)-1} | {time_sec:.2f}s\"\n        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3)\n        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n\n        # Mark if this is a detected contact\n        for c in contacts:\n            if c['frame'] == frame_num:\n                marker = f\"DETECTED: {c['confidence']:.0%} confidence\"\n                color = (0, 255, 0) if c['confidence'] >= 0.7 else (0, 165, 255)\n                cv2.putText(frame, marker, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)\n                cv2.putText(frame, marker, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n\n        # Show ball if detected\n        if frame_num in ball_detections:\n            bx, by, bconf = ball_detections[frame_num]\n            cv2.circle(frame, (int(bx), int(by)), 15, (0, 255, 0), 2)\n            cv2.circle(frame, (int(bx), int(by)), 5, (0, 255, 0), -1)\n\n        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])\n        b64 = b64encode(buffer).decode()\n\n        with output:\n            clear_output(wait=True)\n            display(HTML(f'<img src=\"data:image/jpeg;base64,{b64}\" width=\"{min(w, 800)}\"/>'))\n\n            # Show suggested contacts\n            if contacts:\n                print(\"\\nSuggested contacts from automatic detection:\")\n                for c in contacts:\n                    status = \"✓\" if c['confidence'] >= 0.7 else \"⚠️\"\n                    print(f\"  {status} Frame {c['frame']} ({c['time']:.2f}s): {c['confidence']:.0%}\")\n\n    def on_slider_change(change):\n        update_display(change['new'])\n\n    def step(delta):\n        new_val = max(0, min(len(frames) - 1, frame_slider.value + delta))\n        frame_slider.value = new_val\n\n    def on_select(button):\n        selected_frame['value'] = frame_slider.value\n        frame_num = frame_slider.value\n\n        # Get ball position\n        from src.contact_detection import get_contact_ball_position\n        ball_pos, ball_method = get_contact_ball_position(frame_num, ball_detections)\n\n        # Update ANALYSIS_DATA\n        new_contact = {\n            'index': 0,\n            'frame': frame_num,\n            'time': frame_num / fps,\n            'confidence': 1.0,\n            'confidence_level': 'MANUAL',\n            'source': 'manual',\n            'ball_pos': ball_pos,\n            'ball_method': ball_method,\n        }\n        ANALYSIS_DATA['contacts'] = [new_contact]\n        ANALYSIS_DATA['contacts_raw'] = [(frame_num, 1.0, 'manual')]\n        ANALYSIS_DATA['needs_manual_selection'] = False\n\n        with output:\n            clear_output(wait=True)\n            print(f\"✓ Selected Frame {frame_num} as contact\")\n            print(f\"  Time: {frame_num / fps:.2f}s\")\n            if ball_pos:\n                print(f\"  Ball position: ({ball_pos[0]:.0f}, {ball_pos[1]:.0f})\")\n            print(\"\\n✓ Proceed to Stage 2 (Cell 5)\")\n\n    frame_slider.observe(on_slider_change, names='value')\n    step_buttons.children[0].on_click(lambda b: step(-10))\n    step_buttons.children[1].on_click(lambda b: step(-1))\n    step_buttons.children[2].on_click(lambda b: step(1))\n    step_buttons.children[3].on_click(lambda b: step(10))\n    select_button.on_click(on_select)\n\n    # Display widgets\n    display(widgets.VBox([\n        widgets.HTML(\"<p><b>Instructions:</b> Use slider or buttons to find the frame where ball hits racket.</p>\"),\n        frame_slider,\n        step_buttons,\n        select_button,\n        output,\n    ]))\n\n    update_display(frame_slider.value)\n\nelse:\n    print(\"\\nTo manually select a contact frame:\")\n    print(\"  1. Set MANUAL_CONTACT_FRAME to the frame number, OR\")\n    print(\"  2. Enable USE_INTERACTIVE_SELECTOR and use the slider\")\n    print(\"\\nTip: Watch the diagnostic video (Cell 3) to identify the contact frame.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5. Stage 2: Spatial Localization (Where is contact?)\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom IPython.display import display, Image as IPImage, HTML\nimport os\nfrom base64 import b64encode\n\nfrom utils.coordinate_transforms import (\n    pelvis_origin_transform, estimate_ground_plane, apply_ground_plane\n)\nfrom src.pose_estimation import PoseEstimator\nfrom src.measurements import compute_measurements, compute_relative_to_landmarks\nfrom src.spatial_localization import localize_contact_point\nfrom src.visualization import (\n    draw_skeleton, draw_contact_point, save_annotated_frame\n)\n\n#@markdown ### Select Contact to Analyze\nCONTACT_INDEX = 0  #@param {type:\"integer\"}\n\nif 'ANALYSIS_DATA' not in dir():\n    raise ValueError(\"Please run Cell 2 first!\")\n\ncontacts = ANALYSIS_DATA['contacts']\nif len(contacts) == 0:\n    raise ValueError(\"No contacts detected. Please use Cell 4 for manual selection.\")\n\nif CONTACT_INDEX < 0 or CONTACT_INDEX >= len(contacts):\n    raise ValueError(f\"Invalid contact index. Valid range: 0-{len(contacts)-1}\")\n\ncontact = contacts[CONTACT_INDEX]\nframe_num = contact['frame']\nball_pos = contact['ball_pos']\nball_method = contact['ball_method']\nshot_type = ANALYSIS_DATA['shot_type']\nframes = ANALYSIS_DATA['frames']\nfps = ANALYSIS_DATA['fps']\n\nprint(\"=\"*60)\nprint(\"STAGE 2: SPATIAL LOCALIZATION (Where is contact?)\")\nprint(\"=\"*60)\nprint(f\"\\nAnalyzing Contact {CONTACT_INDEX + 1}\")\nprint(f\"  Frame: {frame_num} ({contact['time']:.2f}s)\")\nprint(f\"  Detection: {contact['confidence']:.0%} confidence ({contact['source']})\")\nprint(f\"  Shot type: {shot_type}\")\n\n# Get frame\nframe = frames[frame_num]\nh, w = frame.shape[:2]\n\n# Determine contact wrist\nif shot_type in [\"right_forehand\", \"right_backhand\"]:\n    contact_wrist_name = \"right_wrist\"\nelse:\n    contact_wrist_name = \"left_wrist\"\n\n# --- Stage 2a: Pose Estimation ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Step 1: Pose Estimation (MediaPipe)\")\npose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\nlandmarks, raw_result = pose_estimator.process_frame(frame)\n\nif landmarks is None:\n    pose_estimator.close()\n    raise ValueError(\"No pose detected. Player may not be visible.\")\n\npixel_lm = pose_estimator.get_pixel_landmarks(raw_result, frame.shape)\npose_estimator.close()\nprint(\"  ✓ Pose detected successfully\")\nprint(f\"  Detected {len(landmarks)} body landmarks\")\n\n# --- Stage 2b: Contact Point Localization ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Step 2: Contact Point Localization\")\n\nloc_result = localize_contact_point(\n    frame=frame,\n    ball_position=ball_pos,\n    pixel_landmarks=pixel_lm,\n    landmarks_3d=landmarks,\n    shot_type=shot_type,\n)\n\ncontact_pixel = loc_result['contact_pixel']\ncontact_method = loc_result['method']\nloc_confidence = loc_result['confidence']\n\nprint(f\"  Method: {contact_method}\")\nprint(f\"  Localization confidence: {loc_confidence:.0%}\")\nif contact_pixel:\n    print(f\"  Contact position (pixels): ({contact_pixel[0]}, {contact_pixel[1]})\")\nif loc_result['racket_center']:\n    print(f\"  Estimated racket center: {loc_result['racket_center']}\")\n\n# --- Stage 2c: 3D Coordinate Transform ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Step 3: Body-Relative 3D Coordinates\")\n\n# Transform to pelvis-centered coordinates\npelvis = landmarks.get(\"pelvis\", np.zeros(3))\ncentered = pelvis_origin_transform(landmarks)\nground_z = estimate_ground_plane(centered)\nadjusted = apply_ground_plane(centered, ground_z)\n\n# Get wrist 3D position as base for contact point\nwrist_3d = landmarks.get(contact_wrist_name, np.zeros(3))\ncontact_3d = wrist_3d.copy()\n\n# Adjust contact 3D based on pixel offset from wrist to ball\nif ball_pos is not None and contact_wrist_name in pixel_lm:\n    wrist_px = pixel_lm[contact_wrist_name]\n    px_offset_x = ball_pos[0] - wrist_px[0]\n    px_offset_y = ball_pos[1] - wrist_px[1]\n\n    # Estimate scale factor from shoulder width\n    left_shoulder = pixel_lm.get(\"left_shoulder\")\n    right_shoulder = pixel_lm.get(\"right_shoulder\")\n    if left_shoulder and right_shoulder:\n        px_shoulder_width = np.sqrt(\n            (right_shoulder[0] - left_shoulder[0])**2 +\n            (right_shoulder[1] - left_shoulder[1])**2\n        )\n        real_shoulder_width = np.linalg.norm(\n            landmarks.get(\"right_shoulder\", np.zeros(3)) -\n            landmarks.get(\"left_shoulder\", np.zeros(3))\n        )\n        scale = real_shoulder_width / (px_shoulder_width + 1e-6)\n    else:\n        scale = 0.001\n\n    contact_3d[0] += px_offset_x * scale\n    contact_3d[1] += px_offset_y * scale\n\ncontact_adjusted = contact_3d - pelvis - np.array([0, 0, ground_z])\n\nprint(f\"  Contact 3D (pelvis-relative): ({contact_adjusted[0]*100:.1f}, {contact_adjusted[1]*100:.1f}, {contact_adjusted[2]*100:.1f}) cm\")\n\n# --- Stage 2d: Compute Measurements ---\nprint(\"\\n\" + \"-\"*60)\nprint(\"Step 4: Computing Measurements\")\n\nmeas = compute_measurements(adjusted, contact_adjusted)\nrel_meas = compute_relative_to_landmarks(landmarks, contact_3d)\nmeas.update(rel_meas)\n\n# Add metadata\nmeas[\"contact_source\"] = f\"{contact['source']} + {contact_method}\"\nmeas[\"detection_confidence\"] = contact['confidence']\nmeas[\"localization_confidence\"] = loc_confidence\nmeas[\"shot_type\"] = shot_type\nmeas[\"frame_num\"] = frame_num\nmeas[\"time_sec\"] = contact['time']\n\n# --- Create annotated frame ---\noutput_dir = \"/content/output\"\nos.makedirs(output_dir, exist_ok=True)\n\nannotated = frame.copy()\nannotated = draw_skeleton(annotated, pixel_lm, thickness=3)\n\nif contact_pixel:\n    cx, cy = int(contact_pixel[0]), int(contact_pixel[1])\n    draw_contact_point(annotated, cx, cy, radius=15)\n    label = f\"CONTACT ({contact_method})\"\n    cv2.putText(annotated, label, (cx + 20, cy - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n# Add frame info\ninfo_text = f\"Frame {frame_num} | {contact['time']:.2f}s | {contact['confidence']:.0%} conf\"\ncv2.putText(annotated, info_text, (20, 30),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)\ncv2.putText(annotated, info_text, (20, 30),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\nout_path = os.path.join(output_dir, f\"contact_{CONTACT_INDEX+1}_frame_{frame_num}.png\")\nsave_annotated_frame(annotated, out_path)\n\n# --- Display Results ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"SPATIAL LOCALIZATION RESULTS\")\nprint(\"=\"*60)\n\n# Show annotated frame\n_, buffer = cv2.imencode('.jpg', annotated, [cv2.IMWRITE_JPEG_QUALITY, 90])\nb64 = b64encode(buffer).decode()\ndisplay(HTML(f'<img src=\"data:image/jpeg;base64,{b64}\" width=\"{min(w, 800)}\"/>'))\n\nprint(f\"\\n{'='*60}\")\nprint(\"CONTACT POINT MEASUREMENTS\")\nprint(f\"{'='*60}\")\nprint(f\"\\nShot type: {shot_type}\")\nprint(f\"Contact source: {meas['contact_source']}\")\n\nprint(f\"\\n--- POSITION (relative to pelvis) ---\")\nprint(f\"  Lateral offset:      {meas.get('lateral_offset_cm', 0):>7.1f} cm  ({meas.get('lateral_offset_inches', 0):>5.1f} in)\")\nprint(f\"  Forward/back:        {meas.get('forward_back_cm', 0):>7.1f} cm  ({meas.get('forward_back_inches', 0):>5.1f} in)\")\nprint(f\"  Height above ground: {meas.get('height_above_ground_cm', 0):>7.1f} cm  ({meas.get('height_above_ground_inches', 0):>5.1f} in)\")\n\nprint(f\"\\n--- CATEGORICAL LABELS ---\")\nprint(f\"  Height category:     {meas.get('height_category', 'N/A')}\")\nprint(f\"  Forward category:    {meas.get('forward_category', 'N/A')}\")\nprint(f\"  Lateral category:    {meas.get('lateral_category', 'N/A')}\")\nprint(f\"  Contact zone:        {meas.get('contact_zone', 'N/A')}\")\n\nif 'cm_in_front_of_shoulder' in meas:\n    print(f\"\\n--- RELATIVE TO SHOULDER ---\")\n    print(f\"  In front of shoulder: {meas.get('cm_in_front_of_shoulder', 0):>6.1f} cm  ({meas.get('inches_in_front_of_shoulder', 0):>5.1f} in)\")\n    print(f\"  Above/below shoulder: {meas.get('cm_above_shoulder', 0):>6.1f} cm  ({meas.get('inches_above_shoulder', 0):>5.1f} in)\")\n\nif 'cm_in_front_of_hip' in meas:\n    print(f\"\\n--- RELATIVE TO HIP ---\")\n    print(f\"  In front of hip:      {meas.get('cm_in_front_of_hip', 0):>6.1f} cm  ({meas.get('inches_in_front_of_hip', 0):>5.1f} in)\")\n    print(f\"  Above/below hip:      {meas.get('cm_above_hip', 0):>6.1f} cm  ({meas.get('inches_above_hip', 0):>5.1f} in)\")\n\nprint(f\"{'='*60}\")\n\n# Save measurements\ncsv_path = os.path.join(output_dir, f\"measurements_contact_{CONTACT_INDEX+1}.csv\")\npd.DataFrame([meas]).to_csv(csv_path, index=False)\nprint(f\"\\nMeasurements saved to: {csv_path}\")\nprint(f\"Annotated frame saved to: {out_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 6. Batch Analysis - Analyze All Contacts\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom utils.coordinate_transforms import (\n    pelvis_origin_transform, estimate_ground_plane, apply_ground_plane\n)\nfrom src.pose_estimation import PoseEstimator\nfrom src.measurements import compute_measurements, compute_relative_to_landmarks\nfrom src.spatial_localization import localize_contact_point\n\nif 'ANALYSIS_DATA' not in dir():\n    raise ValueError(\"Please run Cell 2 first!\")\n\ncontacts = ANALYSIS_DATA['contacts']\nframes = ANALYSIS_DATA['frames']\nfps = ANALYSIS_DATA['fps']\nshot_type = ANALYSIS_DATA['shot_type']\nball_detections = ANALYSIS_DATA['ball_detections']\n\nprint(\"=\"*60)\nprint(\"BATCH ANALYSIS - All Contacts\")\nprint(\"=\"*60)\n\nif len(contacts) == 0:\n    print(\"\\nNo contacts to analyze.\")\nelse:\n    print(f\"\\nAnalyzing {len(contacts)} contacts...\\n\")\n\n    all_measurements = []\n    pose_estimator = PoseEstimator(static_image_mode=True, model_complexity=2)\n\n    if shot_type in [\"right_forehand\", \"right_backhand\"]:\n        contact_wrist_name = \"right_wrist\"\n    else:\n        contact_wrist_name = \"left_wrist\"\n\n    for contact in contacts:\n        idx = contact['index']\n        frame_num = contact['frame']\n        ball_pos = contact['ball_pos']\n        ball_method = contact['ball_method']\n\n        print(f\"Contact {idx+1}: Frame {frame_num}...\", end=\" \")\n\n        frame = frames[frame_num]\n        landmarks, raw_result = pose_estimator.process_frame(frame)\n\n        if landmarks is None:\n            print(\"SKIPPED (no pose)\")\n            continue\n\n        pixel_lm = pose_estimator.get_pixel_landmarks(raw_result, frame.shape)\n\n        # Spatial localization\n        loc_result = localize_contact_point(\n            frame=frame,\n            ball_position=ball_pos,\n            pixel_landmarks=pixel_lm,\n            landmarks_3d=landmarks,\n            shot_type=shot_type,\n        )\n\n        contact_pixel = loc_result['contact_pixel']\n        if contact_pixel is None:\n            print(\"SKIPPED (no contact point)\")\n            continue\n\n        # Transform coordinates\n        pelvis = landmarks.get(\"pelvis\", np.zeros(3))\n        centered = pelvis_origin_transform(landmarks)\n        ground_z = estimate_ground_plane(centered)\n        adjusted = apply_ground_plane(centered, ground_z)\n\n        wrist_3d = landmarks.get(contact_wrist_name, np.zeros(3))\n        contact_3d = wrist_3d.copy()\n\n        if ball_pos is not None and contact_wrist_name in pixel_lm:\n            wrist_px = pixel_lm[contact_wrist_name]\n            px_offset_x = ball_pos[0] - wrist_px[0]\n            px_offset_y = ball_pos[1] - wrist_px[1]\n\n            left_shoulder = pixel_lm.get(\"left_shoulder\")\n            right_shoulder = pixel_lm.get(\"right_shoulder\")\n            if left_shoulder and right_shoulder:\n                px_shoulder_width = np.sqrt(\n                    (right_shoulder[0] - left_shoulder[0])**2 +\n                    (right_shoulder[1] - left_shoulder[1])**2\n                )\n                real_shoulder_width = np.linalg.norm(\n                    landmarks.get(\"right_shoulder\", np.zeros(3)) -\n                    landmarks.get(\"left_shoulder\", np.zeros(3))\n                )\n                scale = real_shoulder_width / (px_shoulder_width + 1e-6)\n            else:\n                scale = 0.001\n\n            contact_3d[0] += px_offset_x * scale\n            contact_3d[1] += px_offset_y * scale\n\n        contact_adjusted = contact_3d - pelvis - np.array([0, 0, ground_z])\n\n        # Compute measurements\n        meas = compute_measurements(adjusted, contact_adjusted)\n        rel_meas = compute_relative_to_landmarks(landmarks, contact_3d)\n        meas.update(rel_meas)\n\n        # Add metadata\n        meas[\"contact_index\"] = idx + 1\n        meas[\"frame_num\"] = frame_num\n        meas[\"time_sec\"] = contact['time']\n        meas[\"detection_confidence\"] = contact['confidence']\n        meas[\"detection_source\"] = contact['source']\n        meas[\"localization_method\"] = loc_result['method']\n        meas[\"localization_confidence\"] = loc_result['confidence']\n        meas[\"shot_type\"] = shot_type\n\n        all_measurements.append(meas)\n        print(f\"OK ({loc_result['method']}, {meas.get('height_category', '?')})\")\n\n    pose_estimator.close()\n\n    if all_measurements:\n        output_dir = \"/content/output\"\n        os.makedirs(output_dir, exist_ok=True)\n        df = pd.DataFrame(all_measurements)\n        csv_path = os.path.join(output_dir, \"all_contacts_measurements.csv\")\n        df.to_csv(csv_path, index=False)\n\n        print(f\"\\n\" + \"=\"*60)\n        print(\"BATCH ANALYSIS SUMMARY\")\n        print(\"=\"*60)\n\n        # Display key columns\n        display_cols = [\n            'contact_index', 'frame_num', 'time_sec',\n            'detection_confidence', 'height_category', 'forward_category',\n            'lateral_offset_cm', 'forward_back_cm', 'height_above_ground_cm'\n        ]\n        display_cols = [c for c in display_cols if c in df.columns]\n        display(df[display_cols])\n\n        print(f\"\\n--- Category Distribution ---\")\n        if 'height_category' in df.columns:\n            print(f\"  Height: {df['height_category'].value_counts().to_dict()}\")\n        if 'forward_category' in df.columns:\n            print(f\"  Forward: {df['forward_category'].value_counts().to_dict()}\")\n\n        print(f\"\\n--- Measurement Ranges ---\")\n        print(f\"  Height: {df['height_above_ground_cm'].min():.1f} - {df['height_above_ground_cm'].max():.1f} cm\")\n        print(f\"  Forward: {df['forward_back_cm'].min():.1f} - {df['forward_back_cm'].max():.1f} cm\")\n        print(f\"  Lateral: {df['lateral_offset_cm'].min():.1f} - {df['lateral_offset_cm'].max():.1f} cm\")\n\n        print(f\"\\nFull results saved to: {csv_path}\")\n    else:\n        print(\"\\nNo measurements computed.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 7. Download All Results\nfrom google.colab import files as colab_files\nimport glob\nimport os\n\noutput_dir = \"/content/output\"\n\nprint(\"=\"*60)\nprint(\"DOWNLOAD RESULTS\")\nprint(\"=\"*60)\n\nall_files = glob.glob(os.path.join(output_dir, \"*\"))\n\nif not all_files:\n    print(\"\\nNo output files found. Run the analysis cells first.\")\nelse:\n    print(\"\\nFiles available for download:\")\n    print(\"-\"*60)\n\n    total_size = 0\n    for f in sorted(all_files):\n        size_mb = os.path.getsize(f) / (1024 * 1024)\n        total_size += size_mb\n        fname = os.path.basename(f)\n        print(f\"  {fname:<40} {size_mb:>6.2f} MB\")\n\n    print(\"-\"*60)\n    print(f\"  {'TOTAL':<40} {total_size:>6.2f} MB\")\n\n    print(\"\\nDownloading all files...\")\n    for f in sorted(all_files):\n        print(f\"  Downloading: {os.path.basename(f)}\")\n        colab_files.download(f)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"Download complete!\")\n    print(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}